{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Comandos para compactar arquivos do drive para depois baixar e descompactar\n",
        "\n",
        "### (usar novamente caso arquivos das pastas automatic-segmentation ou do inquérito em questão forem modificados)\n",
        "\n",
        "* !cd /content/drive/MyDrive/; tar -zcvf SP_D2_255_segmentado.tar.gz SP_D2_255_segmentado\n",
        "\n",
        "* !cd /content/drive/MyDrive/; tar -zcvf automatic-segmentation.tar.gz automatic-segmentation\n",
        "\n"
      ],
      "metadata": {
        "id": "46z-4Bm6e-5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1-EpqeWanszqcpLTBFYXXdXHlrC7HT0Aj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L954IqH0qQIj",
        "outputId": "06e7c9bf-0b1f-489e-e60b-ca716e456fe7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-EpqeWanszqcpLTBFYXXdXHlrC7HT0Aj\n",
            "To: /content/automatic-segmentation.tar.gz\n",
            "100% 215M/215M [00:01<00:00, 167MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xzvf automatic-segmentation.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFEkKkubq7uP",
        "outputId": "aac468aa-4f87-42bd-f1b1-00e25a5d9505"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "automatic-segmentation/\n",
            "automatic-segmentation/NURCSPTranscriptConverter.py\n",
            "automatic-segmentation/pom.xml\n",
            "automatic-segmentation/requirements.txt\n",
            "automatic-segmentation/GramaticalClass.py\n",
            "automatic-segmentation/server.py\n",
            "automatic-segmentation/Procfile\n",
            "automatic-segmentation/manifest.json\n",
            "automatic-segmentation/Parser.py\n",
            "automatic-segmentation/phoneme_rules.py\n",
            "automatic-segmentation/recog.out\n",
            "automatic-segmentation/NURCSP_convert_transcripts.py\n",
            "automatic-segmentation/erros.txt\n",
            "automatic-segmentation/LICENSE\n",
            "automatic-segmentation/README.md\n",
            "automatic-segmentation/Segmentador.py\n",
            "automatic-segmentation/segments.csv\n",
            "automatic-segmentation/corpus/\n",
            "automatic-segmentation/htk/\n",
            "automatic-segmentation/htk/README\n",
            "automatic-segmentation/htk/LICENSE\n",
            "automatic-segmentation/htk/HLMTools/\n",
            "automatic-segmentation/htk/HLMTools/LNewMap.c\n",
            "automatic-segmentation/htk/HLMTools/LPlex.c\n",
            "automatic-segmentation/htk/HLMTools/LMerge.c\n",
            "automatic-segmentation/htk/HLMTools/LNorm.c\n",
            "automatic-segmentation/htk/HLMTools/LSubset.c\n",
            "automatic-segmentation/htk/HLMTools/LGCopy.c\n",
            "automatic-segmentation/htk/HLMTools/LLink.c\n",
            "automatic-segmentation/htk/HLMTools/LFoF.c\n",
            "automatic-segmentation/htk/HLMTools/MakefileMKL\n",
            "automatic-segmentation/htk/HLMTools/LGList.c\n",
            "automatic-segmentation/htk/HLMTools/MakefileCPU\n",
            "automatic-segmentation/htk/HLMTools/HLMCopy.c\n",
            "automatic-segmentation/htk/HLMTools/LGPrep.c\n",
            "automatic-segmentation/htk/HLMTools/LAdapt.c\n",
            "automatic-segmentation/htk/HLMTools/MakefileNVCC\n",
            "automatic-segmentation/htk/HLMTools/LBuild.c\n",
            "automatic-segmentation/htk/HLMTools/Cluster.c\n",
            "automatic-segmentation/htk/HLMLib/\n",
            "automatic-segmentation/htk/HLMLib/LWMap.h\n",
            "automatic-segmentation/htk/HLMLib/MakefileNVCC\n",
            "automatic-segmentation/htk/HLMLib/LCMap.h\n",
            "automatic-segmentation/htk/HLMLib/LGBase.c\n",
            "automatic-segmentation/htk/HLMLib/LModel.c\n",
            "automatic-segmentation/htk/HLMLib/LPMerge.c\n",
            "automatic-segmentation/htk/HLMLib/MakefileCPU\n",
            "automatic-segmentation/htk/HLMLib/LWMap.c\n",
            "automatic-segmentation/htk/HLMLib/LPCalc.h\n",
            "automatic-segmentation/htk/HLMLib/LPMerge.h\n",
            "automatic-segmentation/htk/HLMLib/LUtil.c\n",
            "automatic-segmentation/htk/HLMLib/LGBase.h\n",
            "automatic-segmentation/htk/HLMLib/LPCalc.c\n",
            "automatic-segmentation/htk/HLMLib/LModel.h\n",
            "automatic-segmentation/htk/HLMLib/MakefileMKL\n",
            "automatic-segmentation/htk/HLMLib/LCMap.c\n",
            "automatic-segmentation/htk/HLMLib/LUtil.h\n",
            "automatic-segmentation/htk/HTKLib/\n",
            "automatic-segmentation/htk/HTKLib/HGraf.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HGraf.o\n",
            "automatic-segmentation/htk/HTKLib/HDict.c\n",
            "automatic-segmentation/htk/HTKLib/HFBLat.c\n",
            "automatic-segmentation/htk/HTKLib/esig_asc.o\n",
            "automatic-segmentation/htk/HTKLib/esig_nat.c\n",
            "automatic-segmentation/htk/HTKLib/HNCache.h\n",
            "automatic-segmentation/htk/HTKLib/HExactMPE.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HFBLat.o\n",
            "automatic-segmentation/htk/HTKLib/HParm.c\n",
            "automatic-segmentation/htk/HTKLib/HArc.o\n",
            "automatic-segmentation/htk/HTKLib/HAudio.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HShell.o\n",
            "automatic-segmentation/htk/HTKLib/HRec.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HParm.h\n",
            "automatic-segmentation/htk/HTKLib/HMath.o\n",
            "automatic-segmentation/htk/HTKLib/esig_edr.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HTrain.o\n",
            "automatic-segmentation/htk/HTKLib/HLat.h\n",
            "automatic-segmentation/htk/HTKLib/HLabel.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HDict.o\n",
            "automatic-segmentation/htk/HTKLib/HMem.o\n",
            "automatic-segmentation/htk/HTKLib/HLabel.o\n",
            "automatic-segmentation/htk/HTKLib/HSigP.h\n",
            "automatic-segmentation/htk/HTKLib/esig_edr.o\n",
            "automatic-segmentation/htk/HTKLib/HLabel.c\n",
            "automatic-segmentation/htk/HTKLib/esignal.o\n",
            "automatic-segmentation/htk/HTKLib/HModel.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HTKLiblv.a\n",
            "automatic-segmentation/htk/HTKLib/HVQ.h\n",
            "automatic-segmentation/htk/HTKLib/HShell.h\n",
            "automatic-segmentation/htk/HTKLib/HMap.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HLabel.h\n",
            "automatic-segmentation/htk/HTKLib/HUtil.c\n",
            "automatic-segmentation/htk/HTKLib/esig_nat.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HArc.c\n",
            "automatic-segmentation/htk/HTKLib/HMem.lv.o\n",
            "automatic-segmentation/htk/HTKLib/config.h\n",
            "automatic-segmentation/htk/HTKLib/HFB.h\n",
            "automatic-segmentation/htk/HTKLib/HNCache.c\n",
            "automatic-segmentation/htk/HTKLib/HFB.c\n",
            "automatic-segmentation/htk/HTKLib/HTrain.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HCUDA.cu\n",
            "automatic-segmentation/htk/HTKLib/HSigP.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HWave.h\n",
            "automatic-segmentation/htk/HTKLib/HLat.c\n",
            "automatic-segmentation/htk/HTKLib/HExactMPE.c\n",
            "automatic-segmentation/htk/HTKLib/HNet.c\n",
            "automatic-segmentation/htk/HTKLib/HUtil.h\n",
            "automatic-segmentation/htk/HTKLib/HExactMPE.o\n",
            "automatic-segmentation/htk/HTKLib/HAdapt.h\n",
            "automatic-segmentation/htk/HTKLib/HFBLat.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HUtil.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HLM.h\n",
            "automatic-segmentation/htk/HTKLib/HRec.h\n",
            "automatic-segmentation/htk/HTKLib/esig_nat.o\n",
            "automatic-segmentation/htk/HTKLib/HAudio.c\n",
            "automatic-segmentation/htk/HTKLib/HTKLib.a\n",
            "automatic-segmentation/htk/HTKLib/HMath.h\n",
            "automatic-segmentation/htk/HTKLib/esig_asc.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HMem.c\n",
            "automatic-segmentation/htk/HTKLib/HParm.o\n",
            "automatic-segmentation/htk/HTKLib/esignal.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HShell.c\n",
            "automatic-segmentation/htk/HTKLib/HRec.o\n",
            "automatic-segmentation/htk/HTKLib/HFBLat.h\n",
            "automatic-segmentation/htk/HTKLib/HModel.h\n",
            "automatic-segmentation/htk/HTKLib/HAdapt.lv.o\n",
            "automatic-segmentation/htk/HTKLib/esig_edr.c\n",
            "automatic-segmentation/htk/HTKLib/HVQ.o\n",
            "automatic-segmentation/htk/HTKLib/HLat.o\n",
            "automatic-segmentation/htk/HTKLib/HWave.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HAudio.o\n",
            "automatic-segmentation/htk/HTKLib/HGraf.c\n",
            "automatic-segmentation/htk/HTKLib/HANNet.c\n",
            "automatic-segmentation/htk/HTKLib/HModel.o\n",
            "automatic-segmentation/htk/HTKLib/HDict.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HExactMPE.h\n",
            "automatic-segmentation/htk/HTKLib/HDict.h\n",
            "automatic-segmentation/htk/HTKLib/HMem.h\n",
            "automatic-segmentation/htk/HTKLib/HMap.o\n",
            "automatic-segmentation/htk/HTKLib/esignal.c\n",
            "automatic-segmentation/htk/HTKLib/HGraf.null.c\n",
            "automatic-segmentation/htk/HTKLib/HUtil.o\n",
            "automatic-segmentation/htk/HTKLib/HANNet.o\n",
            "automatic-segmentation/htk/HTKLib/HAudio.h\n",
            "automatic-segmentation/htk/HTKLib/HCUDA.h\n",
            "automatic-segmentation/htk/HTKLib/HShell.lv.o\n",
            "automatic-segmentation/htk/HTKLib/MakefileMKL\n",
            "automatic-segmentation/htk/HTKLib/HAdapt.c\n",
            "automatic-segmentation/htk/HTKLib/HNet.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HRec.c\n",
            "automatic-segmentation/htk/HTKLib/HWave.o\n",
            "automatic-segmentation/htk/HTKLib/esig_asc.c\n",
            "automatic-segmentation/htk/HTKLib/HANNet.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HArc.h\n",
            "automatic-segmentation/htk/HTKLib/HMap.h\n",
            "automatic-segmentation/htk/HTKLib/HNet.h\n",
            "automatic-segmentation/htk/HTKLib/HLM.c\n",
            "automatic-segmentation/htk/HTKLib/HTrain.h\n",
            "automatic-segmentation/htk/HTKLib/HSigP.c\n",
            "automatic-segmentation/htk/HTKLib/HANNet.h\n",
            "automatic-segmentation/htk/HTKLib/HLM.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HVQ.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HMap.c\n",
            "automatic-segmentation/htk/HTKLib/HLat.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HGraf.h\n",
            "automatic-segmentation/htk/HTKLib/HWave.c\n",
            "automatic-segmentation/htk/HTKLib/MakefileNVCC\n",
            "automatic-segmentation/htk/HTKLib/MakefileCPU\n",
            "automatic-segmentation/htk/HTKLib/HNet.o\n",
            "automatic-segmentation/htk/HTKLib/HFB.o\n",
            "automatic-segmentation/htk/HTKLib/HArc.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HLM.o\n",
            "automatic-segmentation/htk/HTKLib/HNCache.o\n",
            "automatic-segmentation/htk/HTKLib/HModel.c\n",
            "automatic-segmentation/htk/HTKLib/HSigP.o\n",
            "automatic-segmentation/htk/HTKLib/HTrain.c\n",
            "automatic-segmentation/htk/HTKLib/HVQ.c\n",
            "automatic-segmentation/htk/HTKLib/HMath.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HMath.c\n",
            "automatic-segmentation/htk/HTKLib/HAdapt.o\n",
            "automatic-segmentation/htk/HTKLib/HNCache.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HParm.lv.o\n",
            "automatic-segmentation/htk/HTKLib/esignal.h\n",
            "automatic-segmentation/htk/HTKLib/HFB.lv.o\n",
            "automatic-segmentation/htk/HTKLib/lib/\n",
            "automatic-segmentation/htk/HTKLib/lib/HTKLiblv.a\n",
            "automatic-segmentation/htk/HTKLib/lib/HTKLib.a\n",
            "automatic-segmentation/htk/samples/\n",
            "automatic-segmentation/htk/samples/LICENSE\n",
            "automatic-segmentation/htk/samples/LMTutorial/\n",
            "automatic-segmentation/htk/samples/LMTutorial/5k.wlist\n",
            "automatic-segmentation/htk/samples/LMTutorial/config\n",
            "automatic-segmentation/htk/samples/LMTutorial/extras/\n",
            "automatic-segmentation/htk/samples/LMTutorial/extras/LCond.pl\n",
            "automatic-segmentation/htk/samples/LMTutorial/extras/60k.wlist\n",
            "automatic-segmentation/htk/samples/LMTutorial/extras/getwordlist.pl\n",
            "automatic-segmentation/htk/samples/LMTutorial/extras/intersection.pl\n",
            "automatic-segmentation/htk/samples/LMTutorial/extras/TagTextWithClassMap.pl\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/scandal_in_bohemia.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/threeudents.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/his_last_bow.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/wisteria_lodge.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/dying_detective.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/reigate_puzzle.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/resident_patient.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/final_problem.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/stock_brokers_clerk.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/red_circle.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/yellow_face.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/secondain.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/black_peter.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/solitary_cyclist.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/abbey_grange.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/six_napoleons.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/empty_house.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/dancing_men.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/case_of_identity.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/silver_blaze.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/hound_of_baskervilles.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/valley_of_fear.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/crooked_man.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/study_in_scarlet.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/devils_foot.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/greek_interpreter.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/golden_pince-nez.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/lady_frances_carfax.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/blue_carbuncle.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/sign_of_four.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/gloria_scott.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/copper_beeches.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/cardboard_box.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/engineers_thumb.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/priory_school.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/charles_agustus_milverton.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/preface.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/bruce-partington_plans.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/norwood_builder.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/man_with_twisted_lip.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/beryl_coronet.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/speckled_band.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/musgrave_ritual.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/five_orange_pips.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/noble_bachelor.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/boscombe_valley.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/missing_three-quarter.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/naval_treaty.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/test/\n",
            "automatic-segmentation/htk/samples/LMTutorial/test/red-headed_league.txt\n",
            "automatic-segmentation/htk/samples/RMHTK/\n",
            "automatic-segmentation/htk/samples/RMHTK/environment\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/latgen-align\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/w_decode\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/coderm\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/hbuild\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/finetune\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/latgen\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/sequence\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/latgen-makelm\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/mkclscript\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/addmlp\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/latgen-decode\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/w_edit\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/latgen-dnn\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/hedit\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/htestrm\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/forward\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/pretrain\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/herest\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/fixrm\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/MakeProtoHMMSet.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/rest_discrete.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/hedit.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/env_conv.pm\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/latgen.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/mk_sub_list.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/mk_mlf.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/discreteMonoSet.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/full_list.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/hbuild.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/coderm.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/ng_net.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/herest.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/init_discrete.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/wp_net.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/global.pl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/fixrm.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/get_ndx.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/mkclscript.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/mk_lab.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/htestrm.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/Carp.pm\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/Exporter.pm\n",
            "automatic-segmentation/htk/samples/RMHTK/work/\n",
            "automatic-segmentation/htk/samples/RMHTK/work/R1/\n",
            "automatic-segmentation/htk/samples/RMHTK/work/R1/README\n",
            "automatic-segmentation/htk/samples/RMHTK/work/R1/varProto\n",
            "automatic-segmentation/htk/samples/RMHTK/work/R1/HTE\n",
            "automatic-segmentation/htk/samples/RMHTK/work/R1/hmm0/\n",
            "automatic-segmentation/htk/samples/RMHTK/work/R1/hmm0/MODELS\n",
            "automatic-segmentation/htk/samples/RMHTK/work/R1/hmm0/MODELS~\n",
            "automatic-segmentation/htk/samples/RMHTK/python_scripts/\n",
            "automatic-segmentation/htk/samples/RMHTK/python_scripts/GenInitDNN.py\n",
            "automatic-segmentation/htk/samples/RMHTK/python_scripts/ConvertExtSCP.py\n",
            "automatic-segmentation/htk/samples/RMHTK/python_scripts/CatHTKFeatures.py\n",
            "automatic-segmentation/htk/samples/RMHTK/python_scripts/SubsetSCP.py\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/regtree_c2.hed\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/sents.snr\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/replacesil.hed\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/eq.ng\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/maketandem.hed\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/corrupt\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/discrete.hmm\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/wordlist\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/discrete.sp\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/quests.hed\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/null.hled\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/eq.wp\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/xwrd.hled\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/dicts/\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/dicts/mono.dct\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/dicts/tri.ded\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/dicts/pcdsril.txt\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/dicts/sil.txt\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/dicts/mono.ded\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/dicts/mono.hd.dct\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/dicts/tri.dct\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/labs/\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/labs/xwrd.hled\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/labs/mono.hled\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/labs/tri.hled\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/awks/\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/awks/full.list.awk\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/awks/ng.net.awk\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/awks/wp.net.awk\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/awks/mlflabs2scp.awk\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.adapt\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.hlr.win\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.mlp.uncompress\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.discrete\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.dnncvn\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.dnn.hd\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.hlm\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.cmllr\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.tandem.basic\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.debug\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.mpe\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.hd.mod\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.dnnbasic\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.xwrd\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.hlm.win\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.dnn.xwrd\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.hlda\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.hd.win\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.tandem.compress\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.code\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.hd\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.semit\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.tandem.xwrd\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.mmi\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.mpe.win\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.mllr\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.mfc.uncompress\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.hlr\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.tandem.semit\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.basic\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.logxwrd\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.cov\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/info/\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/info/ident_MFCC_E_D_A_Z_cvn\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.dnn.decode\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.hlda\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.dnn.am\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.semit\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.align\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.mpe\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.tandem.semit\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.sat.xform\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.align.trn\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.sat.model\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.dnn.latgen\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.dnn.mpe\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.latgen\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.dnn.bn\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.latgen.3\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-17\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-5\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-4\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-15\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-0\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-10\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-11\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-3\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-1\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-9\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-12\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-2\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-13\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-7\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-8\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-16\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-14\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-6\n",
            "automatic-segmentation/htk/samples/HTKDemo/\n",
            "automatic-segmentation/htk/samples/HTKDemo/test.scr\n",
            "automatic-segmentation/htk/samples/HTKDemo/ChangeLog\n",
            "automatic-segmentation/htk/samples/HTKDemo/runDemo.pl\n",
            "automatic-segmentation/htk/samples/HTKDemo/README.NT\n",
            "automatic-segmentation/htk/samples/HTKDemo/MakeProtoHMMSet\n",
            "automatic-segmentation/htk/samples/HTKDemo/README\n",
            "automatic-segmentation/htk/samples/HTKDemo/train.scr\n",
            "automatic-segmentation/htk/samples/HTKDemo/monlabs.scr\n",
            "automatic-segmentation/htk/samples/HTKDemo/runDemo\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata.scr\n",
            "automatic-segmentation/htk/samples/HTKDemo/outaudio\n",
            "automatic-segmentation/htk/samples/HTKDemo/accs.scr\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/triTiedStateS1.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monPlainM1S1.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monDiscM64S3Tree.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/triPlainS1.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/rbiPlainS1.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monSharedM1S3.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monPlainM1S3FullCov.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monDiscM64S1Tree.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monPlainM4S1.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/triDiscM64S3HSmooth.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monTiedMixS1.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/triSharedS1.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/triDiscM64S3.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/triTiedMixS1.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/lbiPlainS1.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/DcfFormat\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monPlainM1S3HERestPell.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monDiscM64S1Lin.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monDiscM64S3Lin.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/triTiedMixS1HSmooth.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/directAudio.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monTiedMixS3.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monPlainM1S3.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/triTiedStateS1.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/triDiscM64S3.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monDiscM64S1Lin.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/triTiedMixS1.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/triPlainS1.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/rbiPlainS1.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monSharedM1S3.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monPlainM1S3.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monPlainM1S3FullCov.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monDiscM64S3Lin.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/triTiedMixS1HSmooth.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/triDiscM64S3HSmooth.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monPlainM1S1.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monPlainM4S1.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/lbiPlainS1.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/triSharedS1.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monDiscM64S1Tree.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monPlainM1S3HERestPell.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monTiedMixS3.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monDiscM64S3Tree.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monTiedMixS1.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/monInternalTie.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/lbiTiedState.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/edllabs.led\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/contDepPlainhs.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/edrlabs.led\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/monInternalTie(1).hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/monSharedhsM1.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/edtlabs(1).led\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/edllabs(1).led\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/lbiTiedState(1).hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/monSharedhsM1(1).hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/edrlabs(1).led\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/edlabs.led\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/contDepTiedhsS3.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/lbiSharedhsM1.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/contDepTiedhsS1.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/monSharedhsM4(1).hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/rbiTiedState.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/rbiSharedhsM1.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/monTiedhsS3(1).hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/edtlabs.led\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/lbiSharedhsM1(1).hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/monTiedhsS1.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/monTiedhsS1(1).hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/triSharedhsM4.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/monTiedhsS3.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/triTiedState.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/monSharedhsM4.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/triSharedhsM1.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/tr4.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/tr2.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/tr7.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/te1.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/tr1.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/tr6.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/tr3.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/te3.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/tr5.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/te2.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/te1.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/tr6.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/tr7.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/tr2.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/tr3.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/tr1.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/tr5.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/te3.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/te2.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/tr4.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/tr6.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/tr5.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/te3.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/te2.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/te1.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/tr1.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/tr4.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/tr7.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/tr2.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/tr3.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/tr5.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/te2.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/tr6.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/tr1.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/tr2.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/tr7.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/te1.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/tr3.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/te3.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/tr4.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/networks/\n",
            "automatic-segmentation/htk/samples/HTKDemo/networks/triNetwork\n",
            "automatic-segmentation/htk/samples/HTKDemo/networks/rbiLattice\n",
            "automatic-segmentation/htk/samples/HTKDemo/networks/triLattice\n",
            "automatic-segmentation/htk/samples/HTKDemo/networks/rbiNetwork\n",
            "automatic-segmentation/htk/samples/HTKDemo/networks/monLattice\n",
            "automatic-segmentation/htk/samples/HTKDemo/networks/monNetwork\n",
            "automatic-segmentation/htk/samples/HTKDemo/networks/lbiLattice\n",
            "automatic-segmentation/htk/samples/HTKDemo/networks/lbiNetwork\n",
            "automatic-segmentation/htk/samples/HTKDemo/codebooks/\n",
            "automatic-segmentation/htk/samples/HTKDemo/codebooks/currentCodebook\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr7.phn\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr3.adc\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/te3.phn\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/te3.adc\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr4.phn\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr3.phn\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr4.adc\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr1.phn\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr6.phn\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr5.phn\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr7.adc\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr6.adc\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/te2.adc\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr1.adc\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr5.adc\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/te1.phn\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/te1.adc\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/te2.phn\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr2.phn\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr2.adc\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/herest_cmn.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hquant.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hinit.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hsmoothCD.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hhed.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hvite_cmn.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hrestVQ.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hrestCD.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hcopy.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hsmoothVQ.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hviteDA.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/herestVQ.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/config.audio\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/herestCD.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hviteVQ.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/herest.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hviteCD.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hsmooth.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hvite.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hrest.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hinitCD.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hcopyDA.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hinitVQ.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hcopyFB.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/example.pcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/proto_s1_m64_vq.pcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/proto_s3_m1_1_1_fc.pcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/proto_s1_m12_dc_tied.pcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/proto_s1_m4_dc.pcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/proto_s3_m64_64_16_vq.pcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/proto_s1_m4_fc.pcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/proto_s1_m1_dc.pcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/proto_s3_m1_1_1_dc.pcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/proto_s1_m1_fc.pcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/contDepList\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/lbiVocab\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/triVocab\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/dataList3\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/dataList1\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/trainFileStubs\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/allPairedFiles\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/rbiVocab\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/bcplist\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/testFileStubs\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/allTrainFiles\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/dataList2\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/bcpvocab\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/test/\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/test/te2.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/test/te1.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/test/te3.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/te3.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/te2.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/tr6.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/tr7.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/tr3.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/tr2.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/te1.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/tr4.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/tr1.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/tr5.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/train/\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/train/tr2.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/train/tr1.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/train/tr3.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/train/tr4.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/train/tr7.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/train/tr5.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/train/tr6.mfc\n",
            "automatic-segmentation/htk/samples/HTKTutorial/\n",
            "automatic-segmentation/htk/samples/HTKTutorial/prompts2mlf\n",
            "automatic-segmentation/htk/samples/HTKTutorial/prompts2wlist\n",
            "automatic-segmentation/htk/samples/HTKTutorial/makesp\n",
            "automatic-segmentation/htk/samples/HTKTutorial/ChangeLog\n",
            "automatic-segmentation/htk/samples/HTKTutorial/maketrihed\n",
            "automatic-segmentation/htk/HTKTools/\n",
            "automatic-segmentation/htk/HTKTools/HMMIRest\n",
            "automatic-segmentation/htk/HTKTools/MakefileNVCC\n",
            "automatic-segmentation/htk/HTKTools/HResults.c\n",
            "automatic-segmentation/htk/HTKTools/HMMIRest.c\n",
            "automatic-segmentation/htk/HTKTools/HNForward.c\n",
            "automatic-segmentation/htk/HTKTools/HLEd\n",
            "automatic-segmentation/htk/HTKTools/HSmooth\n",
            "automatic-segmentation/htk/HTKTools/HCompV.c\n",
            "automatic-segmentation/htk/HTKTools/HQuant\n",
            "automatic-segmentation/htk/HTKTools/HInit.c\n",
            "automatic-segmentation/htk/HTKTools/HBuild.c\n",
            "automatic-segmentation/htk/HTKTools/HList\n",
            "automatic-segmentation/htk/HTKTools/HSGen.c\n",
            "automatic-segmentation/htk/HTKTools/HLEd.c\n",
            "automatic-segmentation/htk/HTKTools/HBuild\n",
            "automatic-segmentation/htk/HTKTools/HRest\n",
            "automatic-segmentation/htk/HTKTools/HVite.c\n",
            "automatic-segmentation/htk/HTKTools/HParse\n",
            "automatic-segmentation/htk/HTKTools/HNTrainSGD\n",
            "automatic-segmentation/htk/HTKTools/HCompV\n",
            "automatic-segmentation/htk/HTKTools/HLRescore\n",
            "automatic-segmentation/htk/HTKTools/HCopy\n",
            "automatic-segmentation/htk/HTKTools/HHEd.c\n",
            "automatic-segmentation/htk/HTKTools/HLConf\n",
            "automatic-segmentation/htk/HTKTools/HResults\n",
            "automatic-segmentation/htk/HTKTools/HNTrainSGD.c\n",
            "automatic-segmentation/htk/HTKTools/HSLab.c\n",
            "automatic-segmentation/htk/HTKTools/HDMan\n",
            "automatic-segmentation/htk/HTKTools/HSGen\n",
            "automatic-segmentation/htk/HTKTools/HParse.c\n",
            "automatic-segmentation/htk/HTKTools/HLStats\n",
            "automatic-segmentation/htk/HTKTools/HCopy.c\n",
            "automatic-segmentation/htk/HTKTools/HVite\n",
            "automatic-segmentation/htk/HTKTools/HERest\n",
            "automatic-segmentation/htk/HTKTools/HNForward\n",
            "automatic-segmentation/htk/HTKTools/HQuant.c\n",
            "automatic-segmentation/htk/HTKTools/HRest.c\n",
            "automatic-segmentation/htk/HTKTools/HSmooth.c\n",
            "automatic-segmentation/htk/HTKTools/HList.c\n",
            "automatic-segmentation/htk/HTKTools/HLRescore.c\n",
            "automatic-segmentation/htk/HTKTools/HInit\n",
            "automatic-segmentation/htk/HTKTools/HLStats.c\n",
            "automatic-segmentation/htk/HTKTools/MakefileMKL\n",
            "automatic-segmentation/htk/HTKTools/HLConf.c\n",
            "automatic-segmentation/htk/HTKTools/HDMan.c\n",
            "automatic-segmentation/htk/HTKTools/HHEd\n",
            "automatic-segmentation/htk/HTKTools/HERest.c\n",
            "automatic-segmentation/htk/HTKTools/MakefileCPU\n",
            "automatic-segmentation/htk/bin.win32/\n",
            "automatic-segmentation/htk/bin.win32/HRest.exe\n",
            "automatic-segmentation/htk/bin.win32/Cluster.exe\n",
            "automatic-segmentation/htk/bin.win32/HHEd.exe\n",
            "automatic-segmentation/htk/bin.win32/LGList.exe\n",
            "automatic-segmentation/htk/bin.win32/HResults.exe\n",
            "automatic-segmentation/htk/bin.win32/LBuild.exe\n",
            "automatic-segmentation/htk/bin.win32/LNewMap.exe\n",
            "automatic-segmentation/htk/bin.win32/HSGen.exe\n",
            "automatic-segmentation/htk/bin.win32/LSubset.exe\n",
            "automatic-segmentation/htk/bin.win32/HCompV.exe\n",
            "automatic-segmentation/htk/bin.win32/LGPrep.exe\n",
            "automatic-segmentation/htk/bin.win32/HInit.exe\n",
            "automatic-segmentation/htk/bin.win32/LLink.exe\n",
            "automatic-segmentation/htk/bin.win32/HERest.exe\n",
            "automatic-segmentation/htk/bin.win32/LPlex.exe\n",
            "automatic-segmentation/htk/bin.win32/HSmooth.exe\n",
            "automatic-segmentation/htk/bin.win32/HVite.exe\n",
            "automatic-segmentation/htk/bin.win32/HLRescore.exe\n",
            "automatic-segmentation/htk/bin.win32/HParse.exe\n",
            "automatic-segmentation/htk/bin.win32/HList.exe\n",
            "automatic-segmentation/htk/bin.win32/LFoF.exe\n",
            "automatic-segmentation/htk/bin.win32/LMerge.exe\n",
            "automatic-segmentation/htk/bin.win32/HLEd.exe\n",
            "automatic-segmentation/htk/bin.win32/HBuild.exe\n",
            "automatic-segmentation/htk/bin.win32/HLStats.exe\n",
            "automatic-segmentation/htk/bin.win32/HMMIRest.exe\n",
            "automatic-segmentation/htk/bin.win32/HSLab.exe\n",
            "automatic-segmentation/htk/bin.win32/HQuant.exe\n",
            "automatic-segmentation/htk/bin.win32/HDMan.exe\n",
            "automatic-segmentation/htk/bin.win32/LGCopy.exe\n",
            "automatic-segmentation/htk/bin.win32/HLMCopy.exe\n",
            "automatic-segmentation/htk/bin.win32/LAdapt.exe\n",
            "automatic-segmentation/htk/bin.win32/LNorm.exe\n",
            "automatic-segmentation/htk/bin.win32/HCopy.exe\n",
            "automatic-segmentation/htk/bin.cpu/\n",
            "automatic-segmentation/htk/bin.cpu/HResults\n",
            "automatic-segmentation/htk/bin.cpu/HRest\n",
            "automatic-segmentation/htk/bin.cpu/HVite\n",
            "automatic-segmentation/htk/bin.cpu/HSGen\n",
            "automatic-segmentation/htk/bin.cpu/HERest\n",
            "automatic-segmentation/htk/bin.cpu/HHEd\n",
            "automatic-segmentation/htk/bin.cpu/HCopy\n",
            "automatic-segmentation/htk/bin.cpu/HParse\n",
            "automatic-segmentation/htk/bin.cpu/HNTrainSGD\n",
            "automatic-segmentation/htk/bin.cpu/HLRescore\n",
            "automatic-segmentation/htk/bin.cpu/HInit\n",
            "automatic-segmentation/htk/bin.cpu/HDMan\n",
            "automatic-segmentation/htk/bin.cpu/HSmooth\n",
            "automatic-segmentation/htk/bin.cpu/HNForward\n",
            "automatic-segmentation/htk/bin.cpu/HQuant\n",
            "automatic-segmentation/htk/bin.cpu/HLEd\n",
            "automatic-segmentation/htk/bin.cpu/HLConf\n",
            "automatic-segmentation/htk/bin.cpu/HMMIRest\n",
            "automatic-segmentation/htk/bin.cpu/HLStats\n",
            "automatic-segmentation/htk/bin.cpu/HCompV\n",
            "automatic-segmentation/htk/bin.cpu/HBuild\n",
            "automatic-segmentation/htk/bin.cpu/HList\n",
            "automatic-segmentation/bin/\n",
            "automatic-segmentation/bin/mfa_generate_dictionary\n",
            "automatic-segmentation/bin/mfa_align\n",
            "automatic-segmentation/bin/mfa_train_and_align\n",
            "automatic-segmentation/bin/mfa_train_g2p\n",
            "automatic-segmentation/__pycache__/\n",
            "automatic-segmentation/__pycache__/Word.cpython-37.pyc\n",
            "automatic-segmentation/__pycache__/hash_map.cpython-37.pyc\n",
            "automatic-segmentation/__pycache__/phoneme_rules.cpython-37.pyc\n",
            "automatic-segmentation/__pycache__/GramaticalClass.cpython-37.pyc\n",
            "automatic-segmentation/__pycache__/Parser.cpython-37.pyc\n",
            "automatic-segmentation/__pycache__/TextConverter.cpython-37.pyc\n",
            "automatic-segmentation/__pycache__/NURCSPTranscriptConverter.cpython-37.pyc\n",
            "automatic-segmentation/__pycache__/Conversor.cpython-37.pyc\n",
            "automatic-segmentation/tmp/\n",
            "automatic-segmentation/tmp/dict\n",
            "automatic-segmentation/tmp/mlf.mlf\n",
            "automatic-segmentation/tmp/mfc.matl\n",
            "automatic-segmentation/tmp/audios.scp\n",
            "automatic-segmentation/tmp/recog.out\n",
            "automatic-segmentation/templates/\n",
            "automatic-segmentation/templates/how.html\n",
            "automatic-segmentation/templates/alinhador.html\n",
            "automatic-segmentation/templates/add.html\n",
            "automatic-segmentation/templates/about.html\n",
            "automatic-segmentation/templates/kaldi.html\n",
            "automatic-segmentation/templates/index.html\n",
            "automatic-segmentation/templates/meta.html\n",
            "automatic-segmentation/lib/\n",
            "automatic-segmentation/lib/_sha256.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_codecs_iso2022.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/binascii.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/select.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/audioop.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/libpython3.6m.so.1.0\n",
            "automatic-segmentation/lib/_pickle.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/fcntl.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/libncursesw.so.6\n",
            "automatic-segmentation/lib/_codecs_tw.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_ssl.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_bz2.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/resource.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/array.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/pyexpat.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_bisect.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_lzma.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/libtinfow.so.6\n",
            "automatic-segmentation/lib/libz.so.1\n",
            "automatic-segmentation/lib/libcrypto.so.1.0.0\n",
            "automatic-segmentation/lib/math.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_datetime.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_socket.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/generate_dictionary\n",
            "automatic-segmentation/lib/_posixsubprocess.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_heapq.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_opcode.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/termios.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_ctypes.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_codecs_kr.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_sha1.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/config\n",
            "automatic-segmentation/lib/align\n",
            "automatic-segmentation/lib/_random.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_struct.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/libpython3.6m.so\n",
            "automatic-segmentation/lib/_codecs_cn.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_codecs_hk.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/libffi.so.6\n",
            "automatic-segmentation/lib/readline.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_sha512.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/mmap.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_multibytecodec.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_codecs_jp.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/grp.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/hmm_names\n",
            "automatic-segmentation/lib/_blake2.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/libreadline.so.7\n",
            "automatic-segmentation/lib/_multiprocessing.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/config.parming\n",
            "automatic-segmentation/lib/libssl.so.1.0.0\n",
            "automatic-segmentation/lib/_decimal.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_hashlib.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_md5.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/base_library.zip\n",
            "automatic-segmentation/lib/grammar\n",
            "automatic-segmentation/lib/_sha3.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/liblzma.so.5\n",
            "automatic-segmentation/lib/train_and_align\n",
            "automatic-segmentation/lib/unicodedata.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/train_g2p\n",
            "automatic-segmentation/lib/zlib.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_struct/\n",
            "automatic-segmentation/lib/_struct/cpython-36m-x86_64-linux-gnu/\n",
            "automatic-segmentation/lib/_struct/cpython-36m-x86_64-linux-gnu/sotruct.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/zlib/\n",
            "automatic-segmentation/lib/zlib/cpython-36m-x86_64-linux-gnu/\n",
            "automatic-segmentation/lib/zlib/cpython-36m-x86_64-linux-gnu/soib.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/include/\n",
            "automatic-segmentation/lib/include/python3.6m/\n",
            "automatic-segmentation/lib/include/python3.6m/pyconfig.h\n",
            "automatic-segmentation/lib/lib/\n",
            "automatic-segmentation/lib/lib/python3.6/\n",
            "automatic-segmentation/lib/lib/python3.6/config-3.6m-x86_64-linux-gnu/\n",
            "automatic-segmentation/lib/lib/python3.6/config-3.6m-x86_64-linux-gnu/Makefile\n",
            "automatic-segmentation/lib/thirdparty/\n",
            "automatic-segmentation/lib/thirdparty/bin/\n",
            "automatic-segmentation/lib/thirdparty/bin/acc-tree-stats\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-base.so\n",
            "automatic-segmentation/lib/thirdparty/bin/feat-to-len\n",
            "automatic-segmentation/lib/thirdparty/bin/build-tree\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-acc-stats-ali\n",
            "automatic-segmentation/lib/thirdparty/bin/farcompilestrings\n",
            "automatic-segmentation/lib/thirdparty/bin/fstcopy\n",
            "automatic-segmentation/lib/thirdparty/bin/ali-to-pdf\n",
            "automatic-segmentation/lib/thirdparty/bin/lattice-align-words\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-latgen-faster\n",
            "automatic-segmentation/lib/thirdparty/bin/compile-questions\n",
            "automatic-segmentation/lib/thirdparty/bin/ali-to-post\n",
            "automatic-segmentation/lib/thirdparty/bin/libfstfar.so.13\n",
            "automatic-segmentation/lib/thirdparty/bin/libfst.so.13\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-global-sum-accs\n",
            "automatic-segmentation/lib/thirdparty/bin/est-lda\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-global-acc-stats\n",
            "automatic-segmentation/lib/thirdparty/bin/est-mllt\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-est\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-sum-accs\n",
            "automatic-segmentation/lib/thirdparty/bin/ivector-extractor-acc-stats\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-info\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-gselect\n",
            "automatic-segmentation/lib/thirdparty/bin/convert-ali\n",
            "automatic-segmentation/lib/thirdparty/bin/feat-to-dim\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-decoder.so\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-hmm.so\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-ivector.so\n",
            "automatic-segmentation/lib/thirdparty/bin/fstcompile\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-align-compiled\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-est-fmllr\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-gmm.so\n",
            "automatic-segmentation/lib/thirdparty/bin/ivector-extract\n",
            "automatic-segmentation/lib/thirdparty/bin/extract-segments\n",
            "automatic-segmentation/lib/thirdparty/bin/apply-cmvn\n",
            "automatic-segmentation/lib/thirdparty/bin/libfstfarscript.so.13\n",
            "automatic-segmentation/lib/thirdparty/bin/ivector-extractor-est\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-init-model\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-cudamatrix.so\n",
            "automatic-segmentation/lib/thirdparty/bin/compute-cmvn-stats\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-global-init-from-feats\n",
            "automatic-segmentation/lib/thirdparty/bin/compute-mfcc-feats\n",
            "automatic-segmentation/lib/thirdparty/bin/acc-lda\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-transform-means\n",
            "automatic-segmentation/lib/thirdparty/bin/compile-train-graphs\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-global-to-fgmm\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-global-get-post\n",
            "automatic-segmentation/lib/thirdparty/bin/draw-tree\n",
            "automatic-segmentation/lib/thirdparty/bin/copy-feats\n",
            "automatic-segmentation/lib/thirdparty/bin/cluster-phones\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-global-est\n",
            "automatic-segmentation/lib/thirdparty/bin/align-equal-compiled\n",
            "automatic-segmentation/lib/thirdparty/bin/ivector-extractor-init\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-init-mono\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-mixup\n",
            "automatic-segmentation/lib/thirdparty/bin/fstarcsort\n",
            "automatic-segmentation/lib/thirdparty/bin/lattice-to-phone-lattice\n",
            "automatic-segmentation/lib/thirdparty/bin/fstdraw\n",
            "automatic-segmentation/lib/thirdparty/bin/compile-train-graphs-fsts\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-boost-silence\n",
            "automatic-segmentation/lib/thirdparty/bin/lattice-oracle\n",
            "automatic-segmentation/lib/thirdparty/bin/ivector-extractor-sum-accs\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-acc-mllt\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-feat.so\n",
            "automatic-segmentation/lib/thirdparty/bin/add-deltas\n",
            "automatic-segmentation/lib/thirdparty/bin/libfstscript.so.13\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-fstext.so\n",
            "automatic-segmentation/lib/thirdparty/bin/append-vector-to-feats\n",
            "automatic-segmentation/lib/thirdparty/bin/compose-transforms\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-nnet2.so\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-adjust-priors\n",
            "automatic-segmentation/lib/thirdparty/bin/ngramsymbols\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-to-raw-nnet\n",
            "automatic-segmentation/lib/thirdparty/bin/libopenblas.so.0\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-get-egs\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-align-compiled\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-am-init\n",
            "automatic-segmentation/lib/thirdparty/bin/matrix-sum-rows\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-insert\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-matrix.so\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-am-info\n",
            "automatic-segmentation/lib/thirdparty/bin/libngramhist.so.134\n",
            "automatic-segmentation/lib/thirdparty/bin/ngrammake\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-get-feature-transform\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-util.so\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-compute-from-egs\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-lat.so\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-compute-prob\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-init\n",
            "automatic-segmentation/lib/thirdparty/bin/linear-to-nbest\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-relabel-egs\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-tree.so\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-am-mixup\n",
            "automatic-segmentation/lib/thirdparty/bin/ngramprint\n",
            "automatic-segmentation/lib/thirdparty/bin/nbest-to-ctm\n",
            "automatic-segmentation/lib/thirdparty/bin/libngram.so.134\n",
            "automatic-segmentation/lib/thirdparty/bin/ngramcount\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-am-average\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-subset-egs\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-am-copy\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-copy-egs\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-shuffle-egs\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-transform.so\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-lm.so\n",
            "automatic-segmentation/lib/thirdparty/bin/show-transitions\n",
            "automatic-segmentation/lib/thirdparty/bin/paste-feats\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-train-transitions\n",
            "automatic-segmentation/lib/thirdparty/bin/splice-feats\n",
            "automatic-segmentation/lib/thirdparty/bin/phonetisaurus-arpa2wfst\n",
            "automatic-segmentation/lib/thirdparty/bin/sum-tree-stats\n",
            "automatic-segmentation/lib/thirdparty/bin/subsample-feats\n",
            "automatic-segmentation/lib/thirdparty/bin/post-to-weights\n",
            "automatic-segmentation/lib/thirdparty/bin/select-feats\n",
            "automatic-segmentation/lib/thirdparty/bin/phonetisaurus-align\n",
            "automatic-segmentation/lib/thirdparty/bin/tree-info\n",
            "automatic-segmentation/lib/thirdparty/bin/weight-silence-post\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-train-parallel\n",
            "automatic-segmentation/lib/thirdparty/bin/scale-post\n",
            "automatic-segmentation/lib/thirdparty/bin/subset-feats\n",
            "automatic-segmentation/lib/thirdparty/bin/transform-feats\n",
            "automatic-segmentation/lib/thirdparty/bin/vector-sum\n",
            "automatic-segmentation/lib/thirdparty/bin/phonetisaurus-g2pfst\n",
            "automatic-segmentation/lib/thirdparty/bin/sum-lda-accs\n",
            "automatic-segmentation/models/\n",
            "automatic-segmentation/models/pt.zip\n",
            "automatic-segmentation/models/hmm/\n",
            "automatic-segmentation/models/hmm/hmmdefs\n",
            "automatic-segmentation/models/hmm/macros\n",
            "automatic-segmentation/Dictionaries/\n",
            "automatic-segmentation/Dictionaries/Homographs.json\n",
            "automatic-segmentation/Dictionaries/Exceptions.txt\n",
            "automatic-segmentation/Dictionaries/Homographs.txt\n",
            "automatic-segmentation/Dictionaries/Exceptions.json\n",
            "automatic-segmentation/Dictionaries/Verbs.json\n",
            "automatic-segmentation/static/\n",
            "automatic-segmentation/static/table.png\n",
            "automatic-segmentation/static/styles/\n",
            "automatic-segmentation/static/styles/style.css\n",
            "automatic-segmentation/hash_map.py\n",
            "automatic-segmentation/Word.py\n",
            "automatic-segmentation/Conversor.py\n",
            "automatic-segmentation/TextConverter.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1-G2k2XJaO_Jvd4mQ37avufXihS0gKRLI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAYHWdVntCJk",
        "outputId": "40c08bca-3e95-4113-b919-8b21edc47053"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-G2k2XJaO_Jvd4mQ37avufXihS0gKRLI\n",
            "To: /content/SP_D2_255_segmentado.tar.gz\n",
            "100% 680M/680M [00:03<00:00, 180MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xzvf SP_D2_255_segmentado.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBULYuBItecJ",
        "outputId": "30ce6b86-2d6b-44c1-f942-dac35f206c9f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SP_D2_255_segmentado/\n",
            "SP_D2_255_segmentado/SP_D2_255.TextGrid\n",
            "SP_D2_255_segmentado/SP_D2_255_1/\n",
            "SP_D2_255_segmentado/SP_D2_255_1/SP_D2_255_clipped_1.wav\n",
            "SP_D2_255_segmentado/SP_D2_255_1/locutores.txt\n",
            "SP_D2_255_segmentado/SP_D2_255_1/new_wavs/\n",
            "SP_D2_255_segmentado/SP_D2_255_1/locutores_palavras_align.txt\n",
            "SP_D2_255_segmentado/SP_D2_255_1/locutores_palavras.txt\n",
            "SP_D2_255_segmentado/SP_D2_255_1/SP_D2_255_clipped_1.lab\n",
            "SP_D2_255_segmentado/SP_D2_255_1/SP_D2_255_clipped_1.mfc\n",
            "SP_D2_255_segmentado/SP_D2_255_3/\n",
            "SP_D2_255_segmentado/SP_D2_255_3/SP_D2_255_clipped_3.wav\n",
            "SP_D2_255_segmentado/SP_D2_255_3/locutores.txt\n",
            "SP_D2_255_segmentado/SP_D2_255_3/new_wavs/\n",
            "SP_D2_255_segmentado/SP_D2_255_8/\n",
            "SP_D2_255_segmentado/SP_D2_255_8/locutores.txt\n",
            "SP_D2_255_segmentado/SP_D2_255_8/SP_D2_255_clipped_8.wav\n",
            "SP_D2_255_segmentado/SP_D2_255_8/new_wavs/\n",
            "SP_D2_255_segmentado/SP_D2_255_7/\n",
            "SP_D2_255_segmentado/SP_D2_255_7/SP_D2_255_clipped_7.wav\n",
            "SP_D2_255_segmentado/SP_D2_255_7/locutores.txt\n",
            "SP_D2_255_segmentado/SP_D2_255_7/new_wavs/\n",
            "SP_D2_255_segmentado/SP_D2_255_4/\n",
            "SP_D2_255_segmentado/SP_D2_255_4/SP_D2_255_clipped_4.wav\n",
            "SP_D2_255_segmentado/SP_D2_255_4/locutores.txt\n",
            "SP_D2_255_segmentado/SP_D2_255_4/new_wavs/\n",
            "SP_D2_255_segmentado/SP_D2_255_2/\n",
            "SP_D2_255_segmentado/SP_D2_255_2/SP_D2_255_clipped_2.wav\n",
            "SP_D2_255_segmentado/SP_D2_255_2/locutores.txt\n",
            "SP_D2_255_segmentado/SP_D2_255_2/new_wavs/\n",
            "SP_D2_255_segmentado/SP_D2_255_6/\n",
            "SP_D2_255_segmentado/SP_D2_255_6/SP_D2_255_clipped_6.wav\n",
            "SP_D2_255_segmentado/SP_D2_255_6/locutores.txt\n",
            "SP_D2_255_segmentado/SP_D2_255_6/new_wavs/\n",
            "SP_D2_255_segmentado/SP_D2_255_5/\n",
            "SP_D2_255_segmentado/SP_D2_255_5/SP_D2_255_clipped_5.wav\n",
            "SP_D2_255_segmentado/SP_D2_255_5/locutores.txt\n",
            "SP_D2_255_segmentado/SP_D2_255_5/new_wavs/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HR2RJeWIsspt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rel_path = \"automatic-segmentation/\"\n",
        "rel_path_inq = \"SP_D2_255_segmentado/\""
      ],
      "metadata": {
        "id": "E_ugdc_QBvaF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sly\n",
        "!pip install py-espeak-ng\n",
        "!pip install phonemizer\n",
        "!pip install pydub\n",
        "!pip install numpy\n",
        "!pip install chardet\n",
        "!pip install pyspellchecker\n",
        "!pip install num2words\n",
        "!pip install librosa\n",
        "!pip install tgt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ3epQeIqxQT",
        "outputId": "4ccff533-6504-4d91-96ed-a35b67a972ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sly\n",
            "  Downloading sly-0.4.tar.gz (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 3.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: sly\n",
            "  Building wheel for sly (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sly: filename=sly-0.4-py3-none-any.whl size=27352 sha256=60282b2c1ac05fb39ac77eb7568f99df04c386c3597c409de9e23354aa167559\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/4b/f0/6c6f1aae74f2cfc4b28ef94b0727b2f10ab0e14c0572378030\n",
            "Successfully built sly\n",
            "Installing collected packages: sly\n",
            "Successfully installed sly-0.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting py-espeak-ng\n",
            "  Downloading py_espeak_ng-0.1.8-py2.py3-none-any.whl (6.3 kB)\n",
            "Installing collected packages: py-espeak-ng\n",
            "Successfully installed py-espeak-ng-0.1.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting phonemizer\n",
            "  Downloading phonemizer-3.2.1-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from phonemizer) (4.1.1)\n",
            "Collecting dlinfo\n",
            "  Downloading dlinfo-1.2.1-py3-none-any.whl (3.6 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from phonemizer) (1.1.0)\n",
            "Collecting segments\n",
            "  Downloading segments-2.2.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: attrs>=18.1 in /usr/local/lib/python3.7/dist-packages (from phonemizer) (22.1.0)\n",
            "Collecting csvw>=1.5.6\n",
            "  Downloading csvw-3.1.1-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting clldutils>=1.7.3\n",
            "  Downloading clldutils-3.12.0-py2.py3-none-any.whl (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 35.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segments->phonemizer) (2022.6.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (0.8.10)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting rfc3986<2\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.10.3)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Collecting language-tags\n",
            "  Downloading language_tags-1.1.0-py2.py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 61.4 MB/s \n",
            "\u001b[?25hCollecting rdflib\n",
            "  Downloading rdflib-6.2.0-py3-none-any.whl (500 kB)\n",
            "\u001b[K     |████████████████████████████████| 500 kB 61.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.3.3)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 530 kB/s \n",
            "\u001b[?25hRequirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from csvw>=1.5.6->segments->phonemizer) (3.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.23.0)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel->csvw>=1.5.6->segments->phonemizer) (2022.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from isodate->csvw>=1.5.6->segments->phonemizer) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (4.12.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (5.9.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->csvw>=1.5.6->segments->phonemizer) (3.8.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from rdflib->csvw>=1.5.6->segments->phonemizer) (3.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->csvw>=1.5.6->segments->phonemizer) (57.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (1.24.3)\n",
            "Installing collected packages: isodate, rfc3986, rdflib, language-tags, colorama, csvw, colorlog, clldutils, segments, dlinfo, phonemizer\n",
            "Successfully installed clldutils-3.12.0 colorama-0.4.5 colorlog-6.7.0 csvw-3.1.1 dlinfo-1.2.1 isodate-0.6.1 language-tags-1.1.0 phonemizer-3.2.1 rdflib-6.2.0 rfc3986-1.5.0 segments-2.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting docopt>=0.6.2\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=5998fd2b0c98fa40e9787ab0c7fd12d7f3f57d5c2cc49dda0680261d5ee733aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, num2words\n",
            "Successfully installed docopt-0.6.2 num2words-0.5.12\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.4.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.21.6)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.56.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: setuptools<60 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (4.12.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa) (3.0.9)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.43.0->librosa) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.43.0->librosa) (3.8.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tgt\n",
            "  Downloading tgt-1.4.4.tar.gz (21 kB)\n",
            "Building wheels for collected packages: tgt\n",
            "  Building wheel for tgt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tgt: filename=tgt-1.4.4-py3-none-any.whl size=28928 sha256=a3bc95da42a96678c86be25053313cb26724b36189d46b4fbe4d6092aecadf49\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/26/00/05f728381a2620ac79029acb7eb117631a8d1046d0c603ab5e\n",
            "Successfully built tgt\n",
            "Installing collected packages: tgt\n",
            "Successfully installed tgt-1.4.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install python-espeak\n",
        "!sudo apt-get update && sudo apt-get install espeak"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHqqpZHRys9C",
        "outputId": "101dd9cf-7468-46b4-dfd3-5a48b0af1c77"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  espeak-data libespeak1 libportaudio2 libsonic0\n",
            "The following NEW packages will be installed:\n",
            "  espeak-data libespeak1 libportaudio2 libsonic0 python-espeak\n",
            "0 upgraded, 5 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 1,166 kB of archives.\n",
            "After this operation, 2,859 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak-data amd64 1.48.04+dfsg-5 [934 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libportaudio2 amd64 19.6.0-1 [64.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsonic0 amd64 0.2.0-6 [13.4 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libespeak1 amd64 1.48.04+dfsg-5 [145 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-espeak amd64 0.5-1build5 [8,932 B]\n",
            "Fetched 1,166 kB in 1s (1,281 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package espeak-data:amd64.\n",
            "(Reading database ... 159447 files and directories currently installed.)\n",
            "Preparing to unpack .../espeak-data_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking espeak-data:amd64 (1.48.04+dfsg-5) ...\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../libsonic0_0.2.0-6_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-6) ...\n",
            "Selecting previously unselected package libespeak1:amd64.\n",
            "Preparing to unpack .../libespeak1_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking libespeak1:amd64 (1.48.04+dfsg-5) ...\n",
            "Selecting previously unselected package python-espeak.\n",
            "Preparing to unpack .../python-espeak_0.5-1build5_amd64.deb ...\n",
            "Unpacking python-espeak (0.5-1build5) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1) ...\n",
            "Setting up espeak-data:amd64 (1.48.04+dfsg-5) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-6) ...\n",
            "Setting up libespeak1:amd64 (1.48.04+dfsg-5) ...\n",
            "Setting up python-espeak (0.5-1build5) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.5) ...\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,990 kB]\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,546 kB]\n",
            "Get:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [913 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,323 kB]\n",
            "Get:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [48.3 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,422 kB]\n",
            "Fetched 11.5 MB in 2s (4,915 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  espeak\n",
            "0 upgraded, 1 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 61.6 kB of archives.\n",
            "After this operation, 209 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak amd64 1.48.04+dfsg-5 [61.6 kB]\n",
            "Fetched 61.6 kB in 0s (155 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package espeak.\n",
            "(Reading database ... 159754 files and directories currently installed.)\n",
            "Preparing to unpack .../espeak_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking espeak (1.48.04+dfsg-5) ...\n",
            "Setting up espeak (1.48.04+dfsg-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp automatic-segmentation/Conversor.py .\n",
        "!cp automatic-segmentation/TextConverter.py .\n",
        "!cp automatic-segmentation/Word.py .\n",
        "!cp automatic-segmentation/Parser.py .\n",
        "!cp automatic-segmentation/GramaticalClass.py .\n",
        "!cp automatic-segmentation/hash_map.py .\n",
        "!cp automatic-segmentation/phoneme_rules.py .\n",
        "!cp automatic-segmentation/Dictionaries/Homographs.json ."
      ],
      "metadata": {
        "id": "7pCn6t7xsJlx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIZ2aI7zmtzF",
        "outputId": "6d189c28-664a-476d-a96f-df6550718091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: 15 shift/reduce conflicts\n"
          ]
        }
      ],
      "source": [
        "from spellchecker import SpellChecker\n",
        "from phonemizer.backend import EspeakBackend\n",
        "from Conversor import *\n",
        "from TextConverter import *\n",
        "import re\n",
        "from num2words import num2words\n",
        "from os import path, listdir\n",
        "import pydub\n",
        "import glob\n",
        "from os.path import isfile, join\n",
        "from collections import OrderedDict\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from scipy.io.wavfile import write\n",
        "import chardet\n",
        "import tgt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_vocab ='ABCDEFGHIJKLMNOPQRSTUVWXYZÇÃÀÁÂÊÉÍÓÔÕÚÛabcdefghijklmnopqrstuvwxyzçãàáâêéíóôõúû()\\-\\'\\n\\?\\./,\\'\\\": '\n",
        "clean_vocab ='ABCDEFGHIJKLMNOPQRSTUVWXYZÇÃÀÁÂÊÉÍÓÔÕÚÛabcdefghijklmnopqrstuvwxyzçãàáâêéíóôõúû\\-\\'\\n\\? '\n",
        "\n",
        "#############################################################\n",
        "# Linked segments lists\n",
        "#############################################################\n",
        "class AudioSegment:\n",
        "  def __init__(self, start, end):\n",
        "    self.start = start\n",
        "    self.end = end\n",
        "    self.next = None\n",
        "    self.gap = 0 # gap between segments (current and next)\n",
        "\n",
        "  def set_next(self, next):\n",
        "    self.next = next\n",
        "    self.gap = next.start - self.end\n",
        "\n",
        "  def set_filename_and_id(self, filename, id):\n",
        "    self.filename = filename\n",
        "    self.id = id\n",
        "\n",
        "  def merge_from(self, next): \n",
        "    # merge two segments (current and next)\n",
        "    self.next = next.next\n",
        "    self.gap = next.gap\n",
        "    self.end = next.end\n",
        "\n",
        "  def duration(self, sample_rate):\n",
        "    return (self.end - self.start - 1) / sample_rate"
      ],
      "metadata": {
        "id": "bQPiTqZZqeKH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutomaticSegmentation:\n",
        "    def __init__(self, path, audio_file, locs_file):\n",
        "        self.path = path\n",
        "        self.audio_file = audio_file\n",
        "        self.locs_file = locs_file\n",
        "        self.text_align = \"\"\n",
        "        self.silences_file = \"\"\n",
        "        self.alignment_tg = \"\"\n",
        "\n",
        "    # palavras_por_locutor\n",
        "    def clean_text(self, new_text):\n",
        "        if new_text[0] == ' ':\n",
        "            new_text = new_text[1:]\n",
        "\n",
        "        new_text = re.sub(\"ininteligível\", \"\", new_text)\n",
        "        new_text = re.sub(\"inint\", \"\", new_text)\n",
        "        new_text = re.sub(\"inint\\.\", \"\", new_text)\n",
        "\n",
        "        # Remove texto entre parênteses duplos\n",
        "        new_text = re.sub(\"\\(\\([^)]*\\)\\)\", \"\", new_text)\n",
        "\n",
        "        # Remove texto entre parênteses duplos e \"...\" (caso o transcritor tenha esquecido de fechar os parênteses)\n",
        "        new_text = re.sub(\"\\(\\([^(\\.\\.\\.)]*\\.\\.\\.\", \"\", new_text)\n",
        "\n",
        "        # Troca :: por espaço\n",
        "        new_text = re.sub(\"::\", \" \", new_text)\n",
        "\n",
        "        # Troca ` por '\n",
        "        new_text = new_text.replace(\"`\",\"'\")\n",
        "\n",
        "        # se não há texto, só pontuação, retornamos a string vazia \"\"\n",
        "        if not re.search('[A-Za-z0-9áàâãéèêíóôõúçÁÀÂÃÉÈÍÓÔÕÚÇ]', new_text):\n",
        "            return \"\"\n",
        "\n",
        "        # Formata conforme o vocabulário limpo\n",
        "        new_text = re.sub(\"[^{}]\".format(clean_vocab), \"\", new_text)\n",
        "\n",
        "        # Remove múltiplos espaços\n",
        "        new_text = re.sub(\"[ ]+\", \" \", new_text)\n",
        "\n",
        "        new_text = re.sub(\"(?<![A-Z])\\.\", \"\", new_text)\n",
        "        new_text = re.sub(\"\\n[ ]+\", \"\\n\", new_text)\n",
        "        new_text = re.sub(\"\\n{3, 6}\", \"\\n\\n\", new_text)\n",
        "        new_text = re.sub(\"[ ]+\", \" \", new_text)\n",
        "\n",
        "        # Substitui ehhhhhh por eh e afins    \n",
        "        new_text = re.sub(\"h+\", \"h\", new_text)\n",
        "\n",
        "        new_text = re.sub(' +', ' ', new_text)\n",
        "        new_text = new_text.replace(\"\\n \", \"\\n\")\n",
        "        \n",
        "        if len(new_text.split(\"\\n\")) > 0:\n",
        "            new_text = os.linesep.join([s for s in new_text.splitlines() if s])\n",
        "        return new_text\n",
        "\n",
        "    def filled_pause_normalization(self, word):\n",
        "        # éh, eh\n",
        "        filled_pause_eh = [\"éh\",\"ehm\",\"ehn\",\"he\",\"éhm\",\"éhn\",\"hé\"]\n",
        "        if word in filled_pause_eh:\n",
        "            word = \"eh\"\n",
        "\n",
        "        # uh, hum, hm, uhm\n",
        "        filled_pause_uh = [\"hum\",\"hm\",\"uhm\",\"hu\",\"uhn\"]\n",
        "        if word in filled_pause_uh:\n",
        "            word = \"uh\"\n",
        "\n",
        "        # uhum, aham\n",
        "        filled_pause_aham = [\"uhum\",\"uhun\",\"unhun\",\"unhum\",\"umhun\",\"umhum\",\n",
        "                             \"hunhun\",\"humhum\",\"hanhan\",\"ahan\",\"uhuhum\"]\n",
        "        if word in filled_pause_aham:\n",
        "            word = \"aham\"\n",
        "\n",
        "        # ah, hã, ãh, ã\n",
        "        filled_pause_ah = [\"hã\",\"ãh\",\"ã\",\"ah\",\"ahn\",\"han\",\"ham\"]\n",
        "        if word in filled_pause_ah:\n",
        "            word = \"ah\"\n",
        "        return word\n",
        "\n",
        "    # Se a palavra não é reconhecida pela lista de palavras da biblioteca, retorna a palavra corrigida.\n",
        "    #  Se não, retorna a mesma palavra passada por parâmetro\n",
        "    def spellcheck(self, word):\n",
        "        spell = SpellChecker(language='pt')\n",
        "        #print(\"spellchecking\", word)\n",
        "\n",
        "        fp_word = self.filled_pause_normalization(word)\n",
        "        if fp_word != word:\n",
        "            return fp_word\n",
        "\n",
        "        #if spell.unknown([word]):\n",
        "        #    if '-' or '?' in word:\n",
        "        #        return word\n",
        "        #    print(\"palavra \\\"\", word, \"\\\" não reconhecida convertida para\", spell.correction(word))\n",
        "        #    return spell.correction(word)\n",
        "        #else:\n",
        "        #    return word\n",
        "        return word\n",
        "\n",
        "    # Gera dois novos arquivos com as falas e os turnos\n",
        "    def generate_words_file(self, locs_file):\n",
        "        with open(locs_file, 'r') as lf:\n",
        "            linhas = lf.readlines()\n",
        "\n",
        "        is_new = True\n",
        "        locs_list = []\n",
        "        for i, l in enumerate(linhas):\n",
        "            if l == '\\n':\n",
        "                break\n",
        "            loc = l.split(';')[0]\n",
        "            if loc not in locs_list:\n",
        "                for ll in locs_list:\n",
        "                    # se o locutor atual é igual a um anterior com a adição ou remoção de um caractere \"-\" ou \".\", é o mesmo locutor, e não adicionamos novamente\n",
        "                    if loc == ll+\".\" or loc == ll+\"-\" or (ll[-1] == \".\" and ll[:len(loc)] == loc) or (ll[-1] == \"-\" and ll[:len(loc)] == loc):\n",
        "                        is_new = False\n",
        "                        print(\"mesmo locutor com nomes diferentes\", loc, ll)\n",
        "                        print(\"trocando\", l)\n",
        "                        linhas[i] = ll + \";\" + l.split(';')[1]\n",
        "                        print(\"por\", l)\n",
        "                        break\n",
        "                if is_new:\n",
        "                    print(\"loc appended\", loc)\n",
        "                    locs_list.append(loc)\n",
        "                is_new = True\n",
        "                        \n",
        "\n",
        "        with open(locs_file.replace(\".txt\", \"_palavras_align.txt\"), 'w') as nlf2:\n",
        "            with open(locs_file.replace(\".txt\", \"_palavras.txt\"), 'w') as nlf:\n",
        "                for l in linhas:\n",
        "                    loc = l.split(\";\")[0]\n",
        "                    l = l.split(\";\")[1]\n",
        "                    l = l.lower()\n",
        "                    # remove indicação do locutor no início da frase\n",
        "                    for iloc in locs_list:\n",
        "                        if iloc in l:\n",
        "                            l = l.replace(iloc, \"\")\n",
        "                    l = self.clean_text(l)\n",
        "                    for lp in l.split():\n",
        "                        #lp = self.spellcheck(lp)\n",
        "                        # se palavra é só um número continua sem escrever\n",
        "                        if lp.isnumeric():\n",
        "                            continue\n",
        "                        nlf.write(loc+';'+lp+\"\\n\")\n",
        "                        nlf2.write(lp+\"\\n\")\n",
        "        self.text_align = locs_file.replace(\".txt\", \"_palavras_align.txt\")\n",
        "\n",
        "    # alinhador_fonetico\n",
        "    def clean_tg(self):\n",
        "\t    if('win' not in sys.platform.lower()):\n",
        "\t\t    subprocess.run(['rm','-f', '*.TextGrid'])\n",
        "\t\t    subprocess.run(['rm','-f','-r','output'])\n",
        "\t    else:\n",
        "\t\t    os.system(\" \".join(['del', '*.TextGrid', '>NUL 2>&1']))\n",
        "\t\t    #subprocess.run(['del',\\\n",
        "\t\t    #\t\t'*.wav',\\\n",
        "\t\t    #\t\t'a',\\\n",
        "\t\t    #\t\t'b',\\\n",
        "\t\t    #\t\t'*.mlf',\\\n",
        "\t\t    #\t\t'dict',\\\n",
        "\t\t    #\t\t'*.out',\\\n",
        "\t\t    #\t\t'*.lab',\\\n",
        "\t\t    #\t\t'*.scp',\\\n",
        "\t\t    #\t\t'*.matl',\\\n",
        "\t\t    #\t\t'*.mfc',\\\n",
        "\t\t    #\t\t'hmmdefs',\\\n",
        "\t\t    #\t\t'*.TextGrid'],\\\n",
        "\t\t    #\t\tstdout=subprocess.DEVNULL,\\\n",
        "\t\t    #\t\tstderr=subprocess.DEVNULL)\n",
        "\n",
        "    def clean(self):\n",
        "        TMP = os.path.join(rel_path,'tmp/')\n",
        "        if('win' not in sys.platform.lower()):\n",
        "            subprocess.run(['rm',\\\n",
        "                    '-f',\\\n",
        "                    os.path.join(TMP,'*.wav'),\\\n",
        "                    os.path.join(TMP,'a'),\\\n",
        "                    os.path.join(TMP,'b'),\\\n",
        "                    os.path.join(TMP,'dict'),\\\n",
        "                    os.path.join(TMP,'*.mlf'),\\\n",
        "                    os.path.join(TMP,'*.out'),\\\n",
        "                    os.path.join(TMP,'*.lab'),\\\n",
        "                    os.path.join(TMP,'*.scp'),\\\n",
        "                    os.path.join(TMP,'*.matl'),\\\n",
        "                    os.path.join(TMP,'*.mfc'),\\\n",
        "                    os.path.join(TMP,'hmmdefs')])\n",
        "            subprocess.run(['rm','-f','-r','output'])\n",
        "        else:\n",
        "            os.system(\" \".join(['del',\\\n",
        "                    os.path.join(TMP,'*.wav'),\\\n",
        "                    os.path.join(TMP,'a'),\\\n",
        "                    os.path.join(TMP,'b'),\\\n",
        "                    os.path.join(TMP,'*.mlf'),\\\n",
        "                    os.path.join(TMP,'dict'),\\\n",
        "                    os.path.join(TMP,'*.out'),\\\n",
        "                    os.path.join(TMP,'*.lab'),\\\n",
        "                    os.path.join(TMP,'*.scp'),\\\n",
        "                    os.path.join(TMP,'*.matl'),\\\n",
        "                    os.path.join(TMP,'*.mfc'),\\\n",
        "                    os.path.join(TMP,'hmmdefs'),\\\n",
        "                    '>NUL 2>&1']))\n",
        "\n",
        "    def align(self, audio_file, text_align):\n",
        "        TC = TextConverter(rel_path)\n",
        "        self.clean_tg()\n",
        "\n",
        "        with open(text_align, 'r') as tf:\n",
        "            text = tf.read()\n",
        "\n",
        "        hmmdefs = False\n",
        "        req_in = 'graf'\n",
        "        req_out = 'fonema'\n",
        "        aligner = 'HTK'\n",
        "\n",
        "        # Cria todos os arquivos necessarios para o alinhamento\t\n",
        "        text1 = TC.perform_conversion(text,\n",
        "                    audio_file,\n",
        "                    req_in,\n",
        "                    req_out,\n",
        "                    rel_path,\n",
        "                    aligner)\n",
        "\n",
        "        TC.align(text, audio_file, req_in, req_out, rel_path, rel_path+\"tmp/\", aligner=aligner, hmmdefs=hmmdefs)\n",
        "        print(\"alinhamento feito\")\n",
        "\n",
        "        # Formata o arquivo de saida para o formato desejado\n",
        "        TC.format_output(text1, audio_file, req_out, rel_path, aligner)\n",
        "        print(\"saidas geradas\")\n",
        "\n",
        "        self.alignment_tg = self.audio_file.replace(\".wav\", \".TextGrid\")\n",
        "\n",
        "        self.clean()\n",
        "\n",
        "    #############################################################\n",
        "    # Segment audio file and return a segment linked list\n",
        "    #############################################################\n",
        "    def segment_wav(self, wav, threshold_db):\n",
        "      # Find gaps at a fine resolution:\n",
        "      parts = librosa.effects.split(wav, top_db=threshold_db, frame_length=1024, hop_length=256)\n",
        "\n",
        "      # Build up a linked list of segments:\n",
        "      head = None\n",
        "      for start, end in parts:\n",
        "        segment = AudioSegment(start, end)\n",
        "        if head is None:\n",
        "          head = segment\n",
        "        else:\n",
        "          prev.set_next(segment)\n",
        "        prev = segment\n",
        "      return head\n",
        "\n",
        "    #############################################################\n",
        "    # Given an audio file, creates the best possible segment list \n",
        "    #############################################################\n",
        "    def find_segments(self, filename, wav, sample_rate, min_duration, max_duration, max_gap_duration, threshold_db, wav_dest_dir):\n",
        "      # Segment audio file\n",
        "      segments = self.segment_wav(wav, threshold_db)\n",
        "\n",
        "      # Convert to list\n",
        "      result = []\n",
        "      s = segments\n",
        "      while s is not None:\n",
        "        result.append(s)\n",
        "        # Create a errors file\n",
        "        if (s.duration(sample_rate) < min_duration and\n",
        "            s.duration(sample_rate) > max_duration):\n",
        "            with open(wav_dest_dir+\"/erros.txt\", \"a\") as f:\n",
        "                f.write(filename+\"\\n\")\n",
        "        s = s.next\n",
        "\n",
        "      with open(wav_dest_dir+'/'+\"silences.txt\", \"w\") as sf:\n",
        "        for r in result:\n",
        "          sf.write(str(r.start/sample_rate) + ' ' + str(r.end/sample_rate) + \"\\n\")\n",
        "\n",
        "      return result\n",
        "\n",
        "    #############################################################\n",
        "    # Given an folder, creates a wav file alphabetical order dict  \n",
        "    #############################################################\n",
        "    def load_filenames(self, base_dir, orig):\n",
        "      mappings = OrderedDict()\n",
        "      for filepath in glob.glob(join(base_dir, orig + \"/*.wav\")):\n",
        "        filename = filepath.split('/')[-1].split('.')[0]\n",
        "        mappings[filename] = filepath\n",
        "      return mappings\n",
        "\n",
        "    #############################################################\n",
        "    # Build best segments of wav files  \n",
        "    #############################################################\n",
        "    def find_silences(self, base_dir, orig, dest, sampling_rate, min_duration, max_duration, max_gap_duration, threshold, output_filename, output_filename_id):\n",
        "      # Creates destination folder\n",
        "      wav_dest_dir = os.path.join(base_dir, dest)\n",
        "      os.makedirs(wav_dest_dir, exist_ok=True)\n",
        "      # Initializes variables\n",
        "      max_duration, mean_duration = 0, 0\n",
        "      all_segments = []\n",
        "      total_duration = 0\n",
        "      filenames = self.load_filenames(base_dir, orig)\n",
        "      for i, (file_id, filename) in enumerate(filenames.items()):\n",
        "        print('Loading %s: %s (%d of %d)' % (file_id, filename, i+1, len(filenames)))\n",
        "        wav, sample_rate = librosa.load(filename, sr=sampling_rate)\n",
        "        print(' -> Loaded %.1f min of audio. Splitting...' % (len(wav) / sample_rate / 60))\n",
        "\n",
        "        # Find best segments\n",
        "        segments = self.find_segments(filename, wav, sample_rate, min_duration, max_duration,\n",
        "          max_gap_duration, threshold, wav_dest_dir)\n",
        "        duration = sum((s.duration(sample_rate) for s in segments))\n",
        "        total_duration += duration\n",
        "\n",
        "        # Create records for the segments\n",
        "        output_filename = output_filename  if output_filename else file_id\n",
        "        j = int(output_filename_id)\n",
        "        for s in segments:\n",
        "          all_segments.append(s)\n",
        "          s.set_filename_and_id(filename, '%s-%04d' % (output_filename, j))\n",
        "          j = j + 1\n",
        "\n",
        "        print(' -> Segmented into %d parts (%.1f min, %.2f sec avg)' % (\n",
        "          len(segments), duration / 60, duration / len(segments)))\n",
        "\n",
        "        # Write segments to disk:\n",
        "        for s in segments:\n",
        "          #segment_wav = (wav[s.start:s.end] * 32767).astype(np.int16)\n",
        "          segment_wav = (wav[s.start:s.end] * 32767).astype(np.int16)\n",
        "          out_path = os.path.join(wav_dest_dir, '%s.wav' % s.id)\n",
        "          #librosa.output.write_wav(out_path, segment_wav, sample_rate)\n",
        "          #write(out_path, sample_rate, segment_wav)\n",
        "\n",
        "          duration += len(segment_wav) / sample_rate\n",
        "          duration_segment = len(segment_wav) / sample_rate\n",
        "          if duration_segment > max_duration:\n",
        "            max_duration = duration_segment\n",
        "\n",
        "          mean_duration = mean_duration + duration_segment\n",
        "        print(' -> Wrote %d segment wav files' % len(segments))\n",
        "        print(' -> Progress: %d segments, %.2f hours, %.2f sec avg' % (\n",
        "          len(all_segments), total_duration / 3600, total_duration / len(all_segments)))\n",
        "\n",
        "      print('Writing metadata for %d segments (%.2f hours)' % (len(all_segments), total_duration / 3600))\n",
        "      with open(os.path.join(base_dir, 'segments.csv'), 'w') as f:\n",
        "        for s in all_segments:\n",
        "          f.write('%s|%s|%d|%d\\n' % (s.id, s.filename, s.start, s.end))\n",
        "      print('Mean: %f' %( mean_duration ))\n",
        "      print('Max: %d' %(max_duration ))\n",
        "      self.silences_file = self.path+\"silences.txt\"\n",
        "\n",
        "    def predict_encoding(self, tg_path):\n",
        "        '''Predict a file's encoding using chardet'''\n",
        "        # Open the file as binary data\n",
        "        with open(tg_path, 'rb') as f:\n",
        "            # Join binary lines for specified number of lines\n",
        "            rawdata = b''.join(f.readlines())\n",
        "\n",
        "        return chardet.detect(rawdata)['encoding']\n",
        "\n",
        "    def calculate_average_phone_duration(self, window_phones):\n",
        "        s = 0\n",
        "        for wp in window_phones:\n",
        "            s += wp[1]\n",
        "        return s / len(window_phones)\n",
        "\n",
        "    def dsr_threshold_1(self, windows, boundaries_tier_1, delta1):\n",
        "        # primeiro encontramos o maior e menor speech rates no turno\n",
        "        max_sr_diff = 0\n",
        "        last_sr = 0\n",
        "        for w in windows:\n",
        "            #print(w)\n",
        "            if abs(w[1] - last_sr) > max_sr_diff:\n",
        "                max_sr_diff = abs(w[1] - last_sr)\n",
        "            last_sr = w[1]\n",
        "\n",
        "        # se a diferença entre os speech rates das janelas consecutivas é > delta1 da maior diferença entre speech rates,\n",
        "        #  identificamos como DSR.\n",
        "        last_sr = 0\n",
        "        dsrs_1 = []\n",
        "        dsr_windows_1 = []\n",
        "        # tempo da última fronteira\n",
        "        last_boundary = 0\n",
        "        for w in windows:\n",
        "            print(abs(w[1] - last_sr), \", threshold is\", delta1, \"*\", max_sr_diff, \"=\", delta1 * max_sr_diff)\n",
        "            if abs(w[1] - last_sr) > delta1 * max_sr_diff:\n",
        "                print(\"DSR!\", w)\n",
        "                print(\"adding boundary to tier 1\")\n",
        "                boundary = tgt.core.Interval(start_time=last_boundary, end_time=w[0][0][2])\n",
        "                last_boundary = w[0][0][2]\n",
        "                try:\n",
        "                    boundaries_tier_1.add_interval(boundary)\n",
        "                except:\n",
        "                    print(\"overlap!\")\n",
        "                dsrs_1.append(w[0][0][2])\n",
        "                dsr_windows_1.append(w)\n",
        "            last_sr = w[1]\n",
        "\n",
        "        return dsrs_1, dsr_windows_1\n",
        "\n",
        "    def dsr_threshold_2(self, dsr_windows_1, boundaries_tier_2, delta2, interval_size):\n",
        "        max_sr = 0\n",
        "        min_sr = 9999\n",
        "        for dsr in dsr_windows_1:\n",
        "            #print(w)\n",
        "            if dsr[1] > max_sr:\n",
        "                max_sr = dsr[1]\n",
        "            if dsr[1] < min_sr:\n",
        "                min_sr = dsr[1]\n",
        "\n",
        "        # se a diferença entre os speech rates das janelas consecutivas é > delta2 da maior diferença entre speech rates,\n",
        "        #  identificamos como DSR.\n",
        "        last_dsr = 0\n",
        "        filtered = []\n",
        "        for dsr in dsr_windows_1:\n",
        "            if dsr[0][0][2] - last_dsr > interval_size:\n",
        "                filtered.append(dsr)\n",
        "            last_dsr = dsr[0][0][3]\n",
        "\n",
        "        dsrs_2 = []\n",
        "        last_sr = 0\n",
        "        last_boundary = 0\n",
        "        for w in filtered:\n",
        "            if abs(w[1] - last_sr) > delta2 * (max_sr - min_sr):\n",
        "                print(\"DSR!\", w)\n",
        "                print(\"adding boundary to tier 2\")\n",
        "                boundary = tgt.core.Interval(start_time=last_boundary, end_time=w[0][0][2])\n",
        "                last_boundary = w[0][0][2]\n",
        "                try:\n",
        "                    boundaries_tier_2.add_interval(boundary)\n",
        "                except:\n",
        "                    print(\"overlap!\")\n",
        "                dsrs_2.append(w[0][0][2])\n",
        "            last_sr = w[1]\n",
        "        return dsrs_2\n",
        "\n",
        "    def print_silences(self, sil_file, boundaries_tier_3, silence_threshold):\n",
        "        with open(sil_file, \"r\") as sf:\n",
        "            sils = sf.readlines()\n",
        "        \n",
        "        silences = []\n",
        "        last_boundary = 0\n",
        "        for s in sils:\n",
        "            #print(\"intervalo de fala\", s)\n",
        "            interval = s.split()\n",
        "            #print(\"trecho de silêncio entre\", last_boundary, \"e\", float(interval[0]), \"com duração\", float(interval[0]) - last_boundary)\n",
        "            if float(interval[0]) - last_boundary > silence_threshold:\n",
        "                print(\"DSR!\", interval[0])\n",
        "                #print(\"adding boundary to tier 3\")\n",
        "                boundary = tgt.core.Interval(start_time=last_boundary, end_time=float(interval[0]))\n",
        "                last_boundary = float(interval[1])\n",
        "                try:\n",
        "                    boundaries_tier_3.add_interval(boundary)\n",
        "                except:\n",
        "                    print(\"overlap!\")\n",
        "                silences.append(float(interval[0]))\n",
        "        return silences\n",
        "\n",
        "    def fill_boundaries_tier(self, timestamps, boundaries_tier):\n",
        "        timestamps.sort()\n",
        "\n",
        "        #print(\"timestamps:\", timestamps)\n",
        "\n",
        "        last_ts = timestamps[0]\n",
        "        for ts in timestamps[1:]:\n",
        "            #print(\"boundary:\", last_ts, ts)\n",
        "            boundary = tgt.core.Interval(start_time=last_ts, end_time=ts)\n",
        "            boundaries_tier.add_interval(boundary)\n",
        "            last_ts = ts\n",
        "\n",
        "    def find_boundaries(self, locs_file, tg_file, sil_file, window_size, delta1, delta2, interval_size, silence_threshold, min_words_h2):\n",
        "        tg = tgt.io.read_textgrid(tg_file, self.predict_encoding(tg_file), include_empty_intervals=False)\n",
        "        tier_names = []\n",
        "        boundaries_tier_1 = tgt.core.IntervalTier(start_time=tg.start_time, end_time=tg.end_time, name=\"fronteiras_heuristica_1\", objects=None)\n",
        "        boundaries_tier_2 = tgt.core.IntervalTier(start_time=tg.start_time, end_time=tg.end_time, name=\"fronteiras_heuristica_2\", objects=None)\n",
        "        boundaries_tier_3 = tgt.core.IntervalTier(start_time=tg.start_time, end_time=tg.end_time, name=\"fronteiras_heuristica_3\", objects=None)\n",
        "        boundaries_tier = tgt.core.IntervalTier(start_time=tg.start_time, end_time=tg.end_time, name=\"fronteiras_metodo\", objects=None)\n",
        "        names = tg.get_tier_names()\n",
        "        fim = False\n",
        "\n",
        "        # lemos as palavras no arquivo com locutores\n",
        "        with open(locs_file, 'r') as lf:\n",
        "            locs_and_words = lf.readlines()\n",
        "\n",
        "            # lemos as palavras na coversão g2p\n",
        "            Conv = Conversor()\n",
        "            sentences = \"\"\n",
        "            for lw in locs_and_words:\n",
        "                w = lw.split(';')[1]\n",
        "                sentences += ' ' + w\n",
        "            #print(\"sentences:\", sentences)\n",
        "            g2p_words = Conv.convert_sentence(sentences, rel_path)\n",
        "            sentences = sentences.split()\n",
        "            #print(\"sentences converted:\", g2p_words)\n",
        "            g2p_words = g2p_words.split()\n",
        "            for pwi, pw in enumerate(g2p_words):\n",
        "                # substituimos fonemas 'w' por 'v' (e 'y' por 'i') pois o alinhador joga fora (é uma solução tosca, mas o que dá pra fazer sem alterar o alinhador)\n",
        "                g2p_words[pwi] = g2p_words[pwi].replace(\"w\", \"v\")\n",
        "                g2p_words[pwi] = g2p_words[pwi].replace(\"y\", \"i\")\n",
        "                \n",
        "            print(\"g2p_words -> lista:\", g2p_words)\n",
        "            \n",
        "        for name in names:\n",
        "            tier = tg.get_tier_by_name(name)\n",
        "            \n",
        "            # índice para iterar pelas palavras convertidas via g2p\n",
        "            i = 0\n",
        "            curr_turn_start = 0.0\n",
        "            curr_turn = \"\"\n",
        "            window_phones = []\n",
        "            tier_names = []\n",
        "            windows = []\n",
        "            all_timestamps = []\n",
        "            constr = \"\"\n",
        "            first_phone = True\n",
        "            curr_word = g2p_words[0]\n",
        "            curr_word_grapheme = sentences[0]\n",
        "            curr_loc = locs_and_words[0].split(';')[0]\n",
        "            turn_index = 0\n",
        "            last_turn_start = 0\n",
        "\n",
        "            # estrutura que guarda, em ordem, cada palavra do texto, o texto do turno atual até dada palavra, o locutor desse turno,\n",
        "            #  o início do tempo da palavra, o início do tempo do turno e o final do tempo do turno até a palavra (final do tempo da palavra)\n",
        "            turn_until_word = []\n",
        "            for w in sentences:\n",
        "                turn_until_word.append([\"\", \"\", \"\", 0.0, 0.0, 0.0])\n",
        "            turn_until_word[0] = [curr_word_grapheme, curr_word_grapheme, curr_loc, 0.0, 0.0, 0.0]            \n",
        "\n",
        "            for enum, interval in enumerate(tier.intervals):\n",
        "                #print(\"fonema:\", interval.text)\n",
        "                if enum == 0 or enum == len(tier.intervals)-1:\n",
        "                    continue\n",
        "                if first_phone:\n",
        "                    curr_window = [interval.start_time, interval.start_time + window_size]\n",
        "                    first_phone = False\n",
        "                \n",
        "                constr += interval.text\n",
        "                if len(constr) < 20:\n",
        "                    print(\"curr_word:\", curr_word, \"; constr:\", constr)\n",
        "                \n",
        "                # adicionamos à lista de fonemas da janela o fonema atual e sua duração caso esteja dentro do tempo da janela\n",
        "                if interval.end_time < curr_window[1]:\n",
        "                    window_phones.append([interval.text, interval.end_time - interval.start_time, interval.start_time, interval.end_time, curr_loc])\n",
        "                \n",
        "                # se os fonemas encontrados desde a última janela formam a próxima palavra, pulamos para a próxima janela\n",
        "                if constr == curr_word:\n",
        "                    curr_turn += ' ' + curr_word_grapheme\n",
        "                    #print(\"window phones:\", window_phones)\n",
        "                    if window_phones:                \n",
        "                        av = self.calculate_average_phone_duration(window_phones)\n",
        "                        windows.append([window_phones, av])\n",
        "\n",
        "                    # atualiza o turno atual até a palavra e o tempo do final do turno até agora\n",
        "                    turn_until_word[i][1] = curr_turn\n",
        "                    turn_until_word[i][5] = interval.end_time\n",
        "                    \n",
        "                    # se a palavra atual foi concluída, pulamos para a próxima\n",
        "                    i += 1\n",
        "                    try:\n",
        "                        curr_word = g2p_words[i]\n",
        "                        curr_word_grapheme = sentences[i]\n",
        "                        #print(curr_word)\n",
        "\n",
        "                        # atualiza a palavra atual no turno e seu tempo de início \n",
        "                        turn_until_word[i][0] = curr_word_grapheme\n",
        "                        turn_until_word[i][3] = interval.end_time\n",
        "                        turn_until_word[i][4] = last_turn_start\n",
        "                        print(\"turn_until_word:\", turn_until_word[i])\n",
        "                    except:\n",
        "                        print(\"lista de g2p acabou\")\n",
        "                        fim = True\n",
        "                        #print(\"turno de\", curr_loc, windows)\n",
        "\n",
        "                        # primeira heurística\n",
        "                        dsrs_1, dsr_windows_1 = self.dsr_threshold_1(windows, boundaries_tier_1, delta1)\n",
        "                        # segunda heurística\n",
        "                        dsrs_2 = self.dsr_threshold_2(dsr_windows_1, boundaries_tier_2, delta2, interval_size)\n",
        "\n",
        "                        # junta todas as fronteiras identificadas pelas duas primeiras heuristicas aplicadas no turno em uma lista\n",
        "                        timestamps = list(set(dsrs_1 + dsrs_2))\n",
        "                        print(\"tamanho dsrs1:\", len(dsrs_1))\n",
        "                        print(\"tamanho dsrs2:\", len(dsrs_2))\n",
        "                        print(\"tamanho timestamps:\", len(timestamps), \"\\n\")\n",
        "                        all_timestamps += timestamps\n",
        "\n",
        "                        # limpa lista de janelas do turno\n",
        "                        windows = []\n",
        "                        break\n",
        "\n",
        "                    print(g2p_words[i], \":\", locs_and_words[i].split(';')[0])\n",
        "                    # se há troca de turno chamamos as heurísticas para as janelas do turno\n",
        "                    if locs_and_words[i].split(';')[0] != curr_loc:\n",
        "                        print(\"turno de\", curr_loc, windows)\n",
        "\n",
        "                        # primeira heurística\n",
        "                        dsrs_1, dsr_windows_1 = self.dsr_threshold_1(windows, boundaries_tier_1, delta1)\n",
        "                        # segunda heurística\n",
        "                        dsrs_2 = self.dsr_threshold_2(dsr_windows_1, boundaries_tier_2, delta2, interval_size)\n",
        "\n",
        "                        # junta todas as fronteiras identificadas pelas duas primeiras heuristicas aplicadas no turno em uma lista\n",
        "                        timestamps = list(set(dsrs_1 + dsrs_2))\n",
        "                        print(\"tamanho dsrs1:\", len(dsrs_1))\n",
        "                        print(\"tamanho dsrs2:\", len(dsrs_2))\n",
        "                        print(\"tamanho timestamps:\", len(timestamps), \"\\n\")\n",
        "                        all_timestamps += timestamps\n",
        "\n",
        "                        # limpa lista de janelas do turno\n",
        "                        windows = []\n",
        "\n",
        "                        if curr_loc in tier_names:\n",
        "                            loc_tb_tier = tg.get_tier_by_name(\"TB-\"+curr_loc)\n",
        "                            loc_ntb_tier = tg.get_tier_by_name(\"NTB-\"+curr_loc)\n",
        "                        else:\n",
        "                            # Creates TB and NTB tiers for the speaker\n",
        "                            loc_tb_tier = tgt.core.IntervalTier(start_time=tg.start_time, end_time=tg.end_time, name=\"TB-\"+curr_loc, objects=None)\n",
        "                            loc_ntb_tier = tgt.core.IntervalTier(start_time=tg.start_time, end_time=tg.end_time, name=\"NTB-\"+curr_loc, objects=None)\n",
        "\n",
        "                            # Adds the new tiers to the textgrid file\n",
        "                            tg.add_tier(loc_tb_tier)\n",
        "                            tg.add_tier(loc_ntb_tier)\n",
        "                            tier_names.append(curr_loc)                   \n",
        "\n",
        "                        # Resets the new tiers variables\n",
        "                        loc_tb_tier = None\n",
        "                        loc_ntb_tier = None\n",
        "                        curr_turn = \"\"\n",
        "\n",
        "                        # atualiza o tempo de início do próximo intervalo\n",
        "                        curr_turn_start = interval.start_time\n",
        "                        print(\"curr_turn_start\", curr_turn_start)\n",
        "                        # atualiza o começo do tempo do turno atual\n",
        "                        turn_until_word[i][4] = curr_turn_start\n",
        "                        last_turn_start = curr_turn_start\n",
        "\n",
        "                    # atualiza locutor para a proxima palavra\n",
        "                    curr_loc = locs_and_words[i].split(';')[0]\n",
        "\n",
        "                    # atualiza o locutor do turno atual, o tempo do início do turno e o texto do turno até a antiga palavra\n",
        "                    turn_until_word[i][2] = curr_loc\n",
        "\n",
        "                    # limpamos a string que guarda a palavra sendo construída pelos fonemas\n",
        "                    constr = \"\"        \n",
        "                    # janelas de 300 ms\n",
        "                    curr_window = [interval.end_time, interval.end_time + window_size]\n",
        "                    # limpamos a lista de fonemas para a próxima janela\n",
        "                    window_phones = []\n",
        "\n",
        "            # terceira heurística\n",
        "            silences = self.print_silences(sil_file, boundaries_tier_3, silence_threshold)\n",
        "\n",
        "            # junta todas as fronteiras identificadas das primeiras heuristicas com a terceira\n",
        "            all_timestamps = list(set(all_timestamps + silences))\n",
        "        \n",
        "            # preenche tier de boundaries juntando as 3 heurísticas\n",
        "            self.fill_boundaries_tier(all_timestamps, boundaries_tier)\n",
        "            \n",
        "        #tg.add_tier(boundaries_tier_1)\n",
        "        #tg.add_tier(boundaries_tier_2)\n",
        "        #tg.add_tier(boundaries_tier_3)\n",
        "\n",
        "        tg.add_tier(boundaries_tier)\n",
        "\n",
        "        last_c = 0\n",
        "        last_text = \"\"\n",
        "        last_loc = turn_until_word[0][2]\n",
        "        last_b = 0.0\n",
        "                \n",
        "        new_intervals = []\n",
        "        # aqui vamos inserir as informações das fronteiras identificadas pelo método nas tiers correspondentes de cada turno no textgrid\n",
        "        for b in boundaries_tier.intervals:\n",
        "            #print(\"boundary:\", b)\n",
        "            # itera pelas palavras\n",
        "            for c, t in enumerate(turn_until_word):\n",
        "                #print(\"inicio de t:\", t[3])\n",
        "                # se o início da fronteira ocorre após o início da palavra atual, pegue o turno até essa palavra\n",
        "                if b.start_time <= t[3]:\n",
        "                    # se trocou de turno zera o último texto\n",
        "                    if t[2] != last_loc:\n",
        "                        # quando troca de turno adicionamos o que resta do texto do turno anterior ao intervalo anterior\n",
        "                        nc = last_c\n",
        "                        while turn_until_word[nc][2] == last_loc:\n",
        "                            nc += 1\n",
        "                        # pega o texto restante do turno anterior após o último trecho de texto para adicionar retroativamente no último intervalo\n",
        "                        tail_text = turn_until_word[nc-1][1][len(last_text):]\n",
        "                        #print(\"turno completo:\", turn_until_word[nc-1][1])\n",
        "                        #print(\"tail text\", tail_text)\n",
        "                        last_i_updated = [tgt.core.Interval(start_time=last_i[0].start_time, end_time=last_i[0].end_time, text=last_i[0].text+' '+tail_text), last_loc]\n",
        "                        new_intervals[last_nic] = last_i_updated\n",
        "\n",
        "                        #print(\"updated:\", last_i_updated[0])\n",
        "\n",
        "                        # cria intervalo com turno até a fronteira\n",
        "                        i = [tgt.core.Interval(start_time=last_b, end_time=b.start_time, text=t[1]), t[2]]\n",
        "\n",
        "                        last_text = \"\"\n",
        "                    else:\n",
        "                        # cria intervalo com turno até a fronteira usando o texto do trecho atual menos o anterior\n",
        "                        i_text = t[1][len(last_text):]\n",
        "                        i = [tgt.core.Interval(start_time=last_b, end_time=b.start_time, text=i_text), t[2]]\n",
        "\n",
        "                    print(\"adicionando intervalo novo\", i)\n",
        "\n",
        "                    # adiciona intervalo nas duas camadas do turno\n",
        "                    new_intervals.append(i)\n",
        "    \n",
        "                    # salva o turno até a fronteira atual e o locutor            \n",
        "                    last_text = t[1]\n",
        "                    last_loc = t[2]\n",
        "                    last_b = b.start_time\n",
        "                    last_i = i\n",
        "                    last_c = c\n",
        "                    last_nic = len(new_intervals)-1\n",
        "\n",
        "                    # para de iterar pelas palavras para essa fronteira pois já foi encontrada\n",
        "                    break\n",
        "\n",
        "        first = True\n",
        "        # incluindo o final do texto no textgrid\n",
        "        if new_intervals[-1][0].end_time != tg.end_time:\n",
        "            nc = last_c+1\n",
        "            curr_i = [tgt.core.Interval(start_time=turn_until_word[last_c][4], end_time=turn_until_word[last_c][3], text=turn_until_word[last_c][1]), turn_until_word[last_c][2]]\n",
        "            while turn_until_word[nc][3] < tg.end_time:\n",
        "                if turn_until_word[nc][2] != last_loc:\n",
        "                    if first:\n",
        "                        curr_i[0].start_time = last_b\n",
        "                        first = False\n",
        "                    if curr_i[0].text != \"\" and curr_i[0].end_time != 0.0:\n",
        "                            new_intervals.append(curr_i)\n",
        "                #print(\"last loop\", turn_until_word[nc], \"last_loc=\", last_loc)\n",
        "                curr_i = [tgt.core.Interval(start_time=turn_until_word[nc][4], end_time=turn_until_word[nc][3], text=turn_until_word[nc][1]), turn_until_word[nc][2]]\n",
        "                last_loc = turn_until_word[nc][2]\n",
        "                nc += 1\n",
        "                if nc >= len(turn_until_word):\n",
        "                    try:\n",
        "                        #print(\"last i\", curr_i)\n",
        "                        if curr_i[0].text != \"\" and curr_i[0].end_time != 0.0:\n",
        "                            new_intervals.append(curr_i)\n",
        "                    except:\n",
        "                        print(\"overlap\")\n",
        "                    break\n",
        "\n",
        "        # adiciona intervalos nas duas camadas do turno adequado\n",
        "        for ni in new_intervals:\n",
        "            tb_turn_tier = tg.get_tier_by_name(\"TB-\"+ni[1])\n",
        "            ntb_turn_tier =  tg.get_tier_by_name(\"NTB-\"+ni[1])\n",
        "\n",
        "            tb_turn_tier.add_interval(ni[0])\n",
        "            ntb_turn_tier.add_interval(ni[0])\n",
        "\n",
        "        #deleta tier com alinhamento fonético e tier original colapsada com fronteiras do metodo\n",
        "        tg.delete_tier(\"labels\")\n",
        "        tg.delete_tier(\"fronteiras_metodo\")\n",
        "\n",
        "        # adiciona tier para comentários dos anotadores\n",
        "        comments_tier = tgt.core.IntervalTier(start_time=tg.start_time, end_time=tg.end_time, name=\"comentarios-anotacao\", objects=None)\n",
        "        tg.add_tier(comments_tier)\n",
        "\n",
        "        for name in tg.get_tier_names():\n",
        "            print(name)\n",
        "\n",
        "        tgt.io.write_to_file(tg, tg_file.replace(\".TextGrid\", \"_novo.TextGrid\"), format='long', encoding='utf-8')\n",
        "\n",
        "    def ser(self, annot_tg, method_tg, boundary_type):\n",
        "        if boundary_type not in [\"TB\", \"NTB\"]:\n",
        "            print(\"boundary_type inválido\")\n",
        "            return 0\n",
        "\n",
        "        Annot_tg = tgt.io.read_textgrid(annot_tg, self.predict_encoding(annot_tg), include_empty_intervals=True)\n",
        "        Method_tg = tgt.io.read_textgrid(method_tg, self.predict_encoding(method_tg), include_empty_intervals=True)\n",
        "        print(\"Method tg\", Method_tg)\n",
        "        \n",
        "        agreement_tier = tgt.core.IntervalTier(start_time=Annot_tg.start_time, end_time=Annot_tg.end_time, name=\"concordancia\", objects=None)\n",
        "\n",
        "        names_annot = Annot_tg.get_tier_names()\n",
        "        names_method = Method_tg.get_tier_names()\n",
        "        \n",
        "        method_boundaries = []\n",
        "        for name in names_method:\n",
        "            tier = Method_tg.get_tier_by_name(name)\n",
        "            if boundary_type in name:\n",
        "                print(\"tier\", name, \"adicionada:\", tier)\n",
        "                for interval in tier.intervals:\n",
        "                    print(\"intervalo do metodo\", interval)\n",
        "                    method_boundaries.append([interval, '0'])\n",
        "                Annot_tg.add_tier(tier)\n",
        "\n",
        "        end_flag = False\n",
        "        I = 0\n",
        "        R = 0\n",
        "        for name in names_annot:\n",
        "            tier = Annot_tg.get_tier_by_name(name)\n",
        "            if boundary_type in name:\n",
        "                for interval in tier.intervals:\n",
        "                    #print(method_boundaries)\n",
        "                    if interval.start_time < method_boundaries[0][0].start_time or interval.start_time > method_boundaries[-1][0].end_time:\n",
        "                        continue\n",
        "                    for mb in method_boundaries:\n",
        "                        if abs(interval.start_time - mb[0].start_time) < 0.01:\n",
        "                            mb[1] = '1'\n",
        "                        else:\n",
        "                            R += 1\n",
        "                        if mb[0].end_time == Method_tg.end_time:\n",
        "                            if abs(interval.end_time - mb[0].end_time) < 0.01:\n",
        "                                end_flag = True\n",
        "        \n",
        "        total = 0\n",
        "        hits = 0\n",
        "        for mb in method_boundaries:\n",
        "            #print(\"concordancia:\", nb)\n",
        "            if mb[1] == '1':\n",
        "                hits += 1\n",
        "            if mb[1] == '0':\n",
        "                I += 1\n",
        "            total += 1\n",
        "\n",
        "        if end_flag:\n",
        "            hits += 1\n",
        "        else:\n",
        "            I += 1\n",
        "\n",
        "        C = hits\n",
        "        SER = (I+R)/(C+R)\n",
        "\n",
        "        print(\"acertos:\", hits, '/', total, '=', hits/total)\n",
        "        print(\"métrica SER:\", '(I+R)/(C+R)', SER)\n",
        "\n",
        "        #Annot_tg.add_tier(agreement_tier)\n",
        "        #tgt.io.write_to_file(Annot_tg, annot_tg.replace(\".TextGrid\", \"_concat.TextGrid\"), format='long', encoding='utf-8')\n",
        "        return SER"
      ],
      "metadata": {
        "id": "tM0JTV0AqeuY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 755 -R ./automatic-segmentation/"
      ],
      "metadata": {
        "id": "tlC_UK2IpglI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = \"SP_D2_255\"\n",
        "segment_number = \"1\"\n",
        "\n",
        "path = rel_path_inq + name + \"_\" + segment_number + \"/\"\n",
        "audio_file = path + name + \"_clipped_\" + segment_number + \".wav\"\n",
        "alignment_tg = path + name + \"_clipped_\" + segment_number + \".TextGrid\"\n",
        "locs_file = path + \"locutores.txt\"\n",
        "locs_file2 = path + \"locutores_palavras.txt\"\n",
        "annot_tg = name + \"_segmentado/\" + name + \".TextGrid\"\n",
        "text_align = path + \"locutores_palavras_align.txt\"\n",
        "silences_file = path + \"new_wavs/silences.txt\"\n",
        "method_tg = path + name + \"_clipped_\" + segment_number + \"_novo.TextGrid\"\n",
        "wavs_path = path + \"new_wavs/\"\n",
        "\n",
        "Segmentation = AutomaticSegmentation(path, audio_file, locs_file)\n",
        "Segmentation.generate_words_file(locs_file)\n",
        "Segmentation.align(audio_file, text_align)\n",
        "# 1 parâmetro: threshold para silêncio: 37.0 (valor em dB, positivo)\n",
        "db_threshold = 37.0\n",
        "Segmentation.find_silences(\"./\", path, wavs_path, 22050, 0.3, 10.0, 5.0, db_threshold, '', 1)\n",
        "# 6 parâmetros: tamanho da janela: 0.3                      (em s, deve ser positivo e não deve ser grande, talvez no max 1s)\n",
        "#               threshold da 1a heurística (porcentagem\n",
        "#                   da maior diferença de taxas de fala\n",
        "#                   de janelas consecutivas para caracterizar\n",
        "#                   DSR): 0.88                              (no intervalo [0, 1] e não muito pequeno, talvez no min 0.5 ou 0.6)\n",
        "#               threshold da 2a heurística: 0.70            (no intervalo [0, 1], sempre abaixo do parâmetro anterior)\n",
        "#               duração de segundos sem DSRs na primeira heuristica para contemplar a 2a: 3 (em s, positivo)\n",
        "#               duração de silêncio para caracterizar pausa: 0.3 (em s, talvez no mínimo 0.15 e no max 1s)\n",
        "#               palavras consecutivas sem DSR na 1a heuristica para contemplar a 2a: 10 (número inteiro talvez entre 5 e 20)\n",
        "window_size = 0.3\n",
        "delta1 = 0.88\n",
        "delta2 = 0.70\n",
        "interval_size = 3\n",
        "silence_threshold = 0.3\n",
        "min_words_h2 = 10\n",
        "Segmentation.find_boundaries(locs_file2, alignment_tg, silences_file, window_size, delta1, delta2, interval_size, silence_threshold, min_words_h2)\n",
        "Segmentation.ser(annot_tg, method_tg, \"NTB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1pm162eqkzp",
        "outputId": "1a76f128-9a65-4ecd-c5c2-04848db7cbd0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method tg <tgt.core.TextGrid object at 0x7fdb69551c50>\n",
            "tier NTB-doc. adicionada: IntervalTier(start_time=0.0, end_time=561.01, name=\"NTB-doc.\", objects=[Interval(0.0, 2.93, \"bem nós gostaríamos de começar esta nossa conversa falando sobre\"), Interval(2.93, 3.47, \"transportes  e viagens\"), Interval(3.47, 73.13124716553288, \"\"), Interval(73.13124716553288, 78.34412698412699, \"eu gostaria de saber como o qual o meio\"), Interval(78.34412698412699, 79.98113378684808, \"de transporte que o senhor se utiliza o professor r\"), Interval(79.98113378684808, 80.78, \"o professor c pa-ra viajar\"), Interval(80.78, 121.67256235827665, \"\"), Interval(121.67256235827665, 123.37922902494331, \"com\"), Interval(123.37922902494331, 124.86530612244898, \"respeito a ao transporte\"), Interval(124.86530612244898, 125.41, \"aéreo vamos\"), Interval(125.41, 125.60834467120182, \"dizer propriamente\"), Interval(125.60834467120182, 125.9566439909297, \"\"), Interval(125.9566439909297, 128.220589569161, \"dito nós gostaríamos que\"), Interval(128.220589569161, 128.81, \"o senhor\"), Interval(128.81, 130.40326530612245, \"nos descrevesse o o atendimento  o funcionamento vamos dizer do\"), Interval(130.40326530612245, 168.98321995464852, \"\"), Interval(168.98321995464852, 173.03510204081633, \"professor\"), Interval(173.03510204081633, 173.56, \"r já viajou de aviao?\"), Interval(173.56, 402.14639455782316, \"\"), Interval(402.14639455782316, 407.31, \"raridade né? agora outros meios de transporte vocês se\"), Interval(407.31, 409.0311111111111, \"utilizam de alguns meios de transportes\"), Interval(409.0311111111111, 410.1108390022676, \"agora existem outros que vocês\"), Interval(410.1108390022676, 410.64489795918365, \"conhecem né?  por mar\"), Interval(410.64489795918365, 561.01, \"\")])\n",
            "intervalo do metodo Interval(0.0, 2.93, \"bem nós gostaríamos de começar esta nossa conversa falando sobre\")\n",
            "intervalo do metodo Interval(2.93, 3.47, \"transportes  e viagens\")\n",
            "intervalo do metodo Interval(3.47, 73.13124716553288, \"\")\n",
            "intervalo do metodo Interval(73.13124716553288, 78.34412698412699, \"eu gostaria de saber como o qual o meio\")\n",
            "intervalo do metodo Interval(78.34412698412699, 79.98113378684808, \"de transporte que o senhor se utiliza o professor r\")\n",
            "intervalo do metodo Interval(79.98113378684808, 80.78, \"o professor c pa-ra viajar\")\n",
            "intervalo do metodo Interval(80.78, 121.67256235827665, \"\")\n",
            "intervalo do metodo Interval(121.67256235827665, 123.37922902494331, \"com\")\n",
            "intervalo do metodo Interval(123.37922902494331, 124.86530612244898, \"respeito a ao transporte\")\n",
            "intervalo do metodo Interval(124.86530612244898, 125.41, \"aéreo vamos\")\n",
            "intervalo do metodo Interval(125.41, 125.60834467120182, \"dizer propriamente\")\n",
            "intervalo do metodo Interval(125.60834467120182, 125.9566439909297, \"\")\n",
            "intervalo do metodo Interval(125.9566439909297, 128.220589569161, \"dito nós gostaríamos que\")\n",
            "intervalo do metodo Interval(128.220589569161, 128.81, \"o senhor\")\n",
            "intervalo do metodo Interval(128.81, 130.40326530612245, \"nos descrevesse o o atendimento  o funcionamento vamos dizer do\")\n",
            "intervalo do metodo Interval(130.40326530612245, 168.98321995464852, \"\")\n",
            "intervalo do metodo Interval(168.98321995464852, 173.03510204081633, \"professor\")\n",
            "intervalo do metodo Interval(173.03510204081633, 173.56, \"r já viajou de aviao?\")\n",
            "intervalo do metodo Interval(173.56, 402.14639455782316, \"\")\n",
            "intervalo do metodo Interval(402.14639455782316, 407.31, \"raridade né? agora outros meios de transporte vocês se\")\n",
            "intervalo do metodo Interval(407.31, 409.0311111111111, \"utilizam de alguns meios de transportes\")\n",
            "intervalo do metodo Interval(409.0311111111111, 410.1108390022676, \"agora existem outros que vocês\")\n",
            "intervalo do metodo Interval(410.1108390022676, 410.64489795918365, \"conhecem né?  por mar\")\n",
            "intervalo do metodo Interval(410.64489795918365, 561.01, \"\")\n",
            "tier NTB-l1 adicionada: IntervalTier(start_time=0.0, end_time=561.01, name=\"NTB-l1\", objects=[Interval(0.0, 3.47, \"\"), Interval(3.47, 5.270929705215419, \"bom eu já viajei pelo país\"), Interval(5.270929705215419, 5.6540589569161, \"inteiro\"), Interval(5.6540589569161, 7.627755102040816, \"ahn dando cursos principalmente como\"), Interval(7.627755102040816, 9.450521541950113, \"autor de livro didático tenho\"), Interval(9.450521541950113, 11.877006802721088, \"a responsabilidade e até a incumbência de dar cursos promovidos\"), Interval(11.877006802721088, 15.638639455782313, \"pelas minhas editoras e nessas circunstâncias eu\"), Interval(15.638639455782313, 17.69360544217687, \"só nao estive no estado do acre\"), Interval(17.69360544217687, 21.0140589569161, \"e nos territórios federais e naturalmente em todos os\"), Interval(21.0140589569161, 21.96, \"demais\"), Interval(21.96, 23.16190476190476, \"estados em\"), Interval(23.16190476190476, 23.637913832199548, \"alguns\"), Interval(23.637913832199548, 25.193650793650793, \"estados e muitas cidades\"), Interval(25.193650793650793, 28.142585034013607, \"do interior e naturalmente na própria capital eh assim em termos\"), Interval(28.142585034013607, 29.68671201814059, \"turisticos eu viajo muito raramente\"), Interval(29.68671201814059, 31.207619047619048, \"até porque essa obrigatoriedade\"), Interval(31.207619047619048, 35.247891156462586, \"de uma viagem sistemática quase todo ja-neiro em termos de\"), Interval(35.247891156462586, 36.57142857142857, \"compromissos com as\"), Interval(36.57142857142857, 38.81215419501134, \"editoras tira -disponibilidade de tempo e\"), Interval(38.81215419501134, 39.19528344671202, \"até interesse em\"), Interval(39.19528344671202, 40.06603174603175, \"viagens de cunho meramente\"), Interval(40.06603174603175, 41.16897959183674, \"turístico o r nao sei se viaja  bastante ou igual proporçao\"), Interval(41.16897959183674, 59.37342403628118, \"\"), Interval(59.37342403628118, 63.75038548752835, \"é no meu caso ocorre uma circunstância curiosa ra-ramente eu passo mais de quarenta\"), Interval(63.75038548752835, 64.33, \"e oito\"), Interval(64.33, 69.65986394557824, \"horas no locais em que viajo por onde passo e dessas quarenta e\"), Interval(69.65986394557824, 73.13124716553288, \"oito ho-ras umas dez eu passo dormindo e umas vinte dando cursos o que reduz assim o espaço para conhecer e  para realmente entrar em contato com o ambiente ahn muito pequeno ahn extremamente reduzido\"), Interval(73.13124716553288, 99.92707482993197, \"\"), Interval(99.92707482993197, 101.15773242630385, \"a minha última\"), Interval(101.15773242630385, 102.43482993197279, \"viagem eu fiz uma cobertura de catorze estados\"), Interval(102.43482993197279, 104.28081632653061, \"em dezessete dias de maneira que nessas\"), Interval(104.28081632653061, 105.44181405895692, \"condiçoes só\"), Interval(105.44181405895692, 108.59972789115646, \"poderia ter sido de aviao realmente o único trecho nao aéreo\"), Interval(108.59972789115646, 110.80562358276644, \"em todo percurso foi um trecho fortaleza-teresina feito durante\"), Interval(110.80562358276644, 114.07963718820862, \"a madrugada de maneira que o aviao acaba sendo\"), Interval(114.07963718820862, 114.95038548752835, \"o meio de transporte único\"), Interval(114.95038548752835, 115.71664399092971, \"possível nas atuais\"), Interval(115.71664399092971, 117.99219954648527, \"circunstâncias mas também o preferido\"), Interval(117.99219954648527, 119.21, \"por mim\"), Interval(119.21, 120.56961451247166, \"eu realmente talvez pela circunstância em\"), Interval(120.56961451247166, 121.67256235827665, \"que viajo venha  a preferir o aviao sobre outro meio\"), Interval(121.67256235827665, 130.40326530612245, \"\"), Interval(130.40326530612245, 133.32897959183674, \"bom\"), Interval(133.32897959183674, 136.02249433106576, \"o atendimento é diretamente proporcional à extensao do do\"), Interval(136.02249433106576, 140.3065759637188, \"vôo e naturalmente o preço da passagem ahn eu quero crer que uma viagem sao paulo e manaus ou sao paulo a belém a gente\"), Interval(140.3065759637188, 145.345306122449, \"costuma ser muito bem atendido e regiamente tratado de maneira assim toda especial em termos particulares eu\"), Interval(145.345306122449, 146.90104308390022, \"prefiro a vasp sempre me dei muito bem com\"), Interval(146.90104308390022, 148.59609977324263, \"o tratamento da vasp mas nao nunca fui\"), Interval(148.59609977324263, 149.37396825396826, \"mal\"), Interval(149.37396825396826, 152.89179138321995, \"tratado ou mal recebido em outras companhias areas agora numa viagem\"), Interval(152.89179138321995, 154.7261678004535, \"daqui ao rio de janeiro que é uma viagem quase que\"), Interval(154.7261678004535, 157.68671201814058, \"habitual e comumaté o cafezinho de péssima qualidade está se\"), Interval(157.68671201814058, 159.1960090702948, \"tornando cada vez mais difícil de maneira que\"), Interval(159.1960090702948, 161.75, \"eu suponho que nesta proporçao e mesmo em contato com\"), Interval(161.75, 161.9243537414966, \"outras\"), Interval(161.9243537414966, 163.97931972789115, \"pessoas que tiveram viagens internacionais que\"), Interval(163.97931972789115, 165.70920634920634, \"à medida que vai a distância aumentando vai\"), Interval(165.70920634920634, 168.98321995464852, \"naturalmente aumentando o preço da passagem em funçao disso a qualificaçao do  tratamento que aí no caso acaba se tornando muito boa e muito produtiva\"), Interval(168.98321995464852, 214.98195011337867, \"\"), Interval(214.98195011337867, 217.4548752834467, \"comigo é o contrário eu talvez\"), Interval(217.4548752834467, 221.48353741496598, \"até por um certo exotismo gostaria de poder de contar alguma experiência\"), Interval(221.48353741496598, 223.06, \"aérea traumatizante ou pelo menos inquietante\"), Interval(223.06, 223.65, \"mas\"), Interval(223.65, 225.93015873015872, \"a nao ser alguns atrasos homéricos né? excepcionais\"), Interval(225.93015873015872, 230.06331065759636, \"assim em termos de viagens aparentemente de pequena distância nunca\"), Interval(230.06331065759636, 231.61904761904762, \"encontrei circunstâncias que tenham me\"), Interval(231.61904761904762, 236.42557823129252, \"feito preocupar ou temer pela própria segurança nunca\"), Interval(236.42557823129252, 236.9596371882086, \"\"), Interval(236.9596371882086, 245.95736961451246, \"\"), Interval(245.95736961451246, 246.46820861678003, \"\"), Interval(246.46820861678003, 246.94421768707483, \"\"), Interval(246.94421768707483, 249.27782312925171, \"\"), Interval(249.27782312925171, 251.5998185941043, \"\"), Interval(251.5998185941043, 252.11065759637188, \"\"), Interval(252.11065759637188, 252.55183673469386, \"\"), Interval(252.55183673469386, 258.7747845804989, \"\"), Interval(258.7747845804989, 260.42340136054423, \"\"), Interval(260.42340136054423, 264.8003628117914, \"\"), Interval(264.8003628117914, 270.0132426303855, \"\"), Interval(270.0132426303855, 273.57750566893424, \"\"), Interval(273.57750566893424, 279.2083446712018, \"\"), Interval(279.2083446712018, 280.4738321995465, \"\"), Interval(280.4738321995465, 292.3740589569161, \"\"), Interval(292.3740589569161, 295.6248526077097, \"\"), Interval(295.6248526077097, 296.10086167800455, \"\"), Interval(296.10086167800455, 300.930612244898, \"\"), Interval(300.930612244898, 301.99873015873015, \"\"), Interval(301.99873015873015, 302.5792290249433, \"\"), Interval(302.5792290249433, 304.1814058956916, \"\"), Interval(304.1814058956916, 311.1241723356009, \"\"), Interval(311.1241723356009, 313.7596371882086, \"\"), Interval(313.7596371882086, 318.24108843537414, \"\"), Interval(318.24108843537414, 320.6675736961451, \"\"), Interval(320.6675736961451, 323.2914285714286, \"\"), Interval(323.2914285714286, 324.11573696145126, \"\"), Interval(324.11573696145126, 324.9168253968254, \"\"), Interval(324.9168253968254, 327.9121995464853, \"\"), Interval(327.9121995464853, 329.3053968253968, \"\"), Interval(329.3053968253968, 333.47337868480724, \"\"), Interval(333.47337868480724, 334.5298866213152, \"\"), Interval(334.5298866213152, 337.7922902494331, \"\"), Interval(337.7922902494331, 338.53532879818596, \"\"), Interval(338.53532879818596, 342.86585034013603, \"tive aquela tao característica posiçao do indivíduo que desce e\"), Interval(342.86585034013603, 344.2358276643991, \"\"), Interval(344.2358276643991, 344.75827664399094, \"beija a\"), Interval(344.75827664399094, 346.60426303854877, \"terra agradecido sempre as minhas\"), Interval(346.60426303854877, 350.5516553287982, \"experiências aéreas foram das mais favoráveis possíveis como particularidade curiosa das minhas viagens aéreas eu suponho que ahn\"), Interval(350.5516553287982, 353.059410430839, \"a mais exótica foi o fato de ter trocado\"), Interval(353.059410430839, 355.5787755102041, \"o valor de uma passagem por uma palestra a oito\"), Interval(355.5787755102041, 357.68018140589567, \"mil metros de altitude quando do vôo inaugural da vasp para manaus ahn\"), Interval(357.68018140589567, 360.4665759637188, \"o vôo do one eleven' um dos diretores\"), Interval(360.4665759637188, 361.01, \"da companhia\"), Interval(361.01, 361.67, \"me\"), Interval(361.67, 364.5532879818594, \"propôs como forma de promoçao uma passagem aérea gratuivta eu eu disse a\"), Interval(364.5532879818594, 366.48054421768705, \"ele que só poderia aceitar se fosse possível\"), Interval(366.48054421768705, 368.7096598639456, \"levar minha esposa\"), Interval(368.7096598639456, 372.3784126984127, \"tambem entao ele disse que nessa circunstância para justificar perante a companhia a ida da\"), Interval(372.3784126984127, 376.1168253968254, \"minha minha esposa eu teria que fazer alguma coisa pela companhia e a aquilo\"), Interval(376.1168253968254, 376.90630385487526, \"que o professor sabe fazer ahn\"), Interval(376.90630385487526, 379.5069387755102, \"única e exclusivamente é dar\"), Interval(379.5069387755102, 381.85215419501134, \"aula entao ele pediu que eu preparasse uma aula\"), Interval(381.85215419501134, 384.67337868480723, \"para apresentar aos passageiros através do do microfone de de bordo ahn\"), Interval(384.67337868480723, 385.8459863945578, \"numa grande altitude\"), Interval(385.8459863945578, 388.87619047619046, \"e foi uma\"), Interval(388.87619047619046, 390.69895691609975, \"palestra de oito minutos nao mais do\"), Interval(390.69895691609975, 392.3591836734694, \"que isso sobre a ocupaçao amazônica do interesse\"), Interval(392.3591836734694, 394.24, \"internacional pela regiao e o\"), Interval(394.24, 398.3151020408163, \"de exótico é que acredito que poucas pessoas tiveram o privilégio que tive de dar\"), Interval(398.3151020408163, 400.78802721088437, \"uma palestra a oito\"), Interval(400.78802721088437, 402.14639455782316, \"mil metros de  altitude\"), Interval(402.14639455782316, 410.64489795918365, \"\"), Interval(410.64489795918365, 412.9320634920635, \"as minhas experiências marítimas sao extremamente\"), Interval(412.9320634920635, 416.27573696145123, \"limitadas eu ahn digamos assim como passeios para conheci-mento da linha náutica\"), Interval(416.27573696145123, 417.2974149659864, \"o oferta assim do ministério\"), Interval(417.2974149659864, 418.76027210884354, \"da marinha algumas\"), Interval(418.76027210884354, 419.06, \"\"), Interval(419.06, 419.07374149659864, \"vezes\"), Interval(419.07374149659864, 420.01, \"\"), Interval(420.01, 421.07065759637186, \"eu tive a oportunidade\"), Interval(421.07065759637186, 421.5698866213152, \"\"), Interval(421.5698866213152, 422.11555555555555, \"\"), Interval(422.11555555555555, 424.9251700680272, \"da fazer ahn a uma incursao pelo mar muito pequena\"), Interval(424.9251700680272, 426.2254875283447, \"o transporte urbano e mesmo o\"), Interval(426.2254875283447, 426.991746031746, \"\"), Interval(426.991746031746, 428.76807256235827, \"transporte suburbano e às vezes até\"), Interval(428.76807256235827, 431.44997732426305, \"interestadual do ônibus muitas vezes acabei\"), Interval(431.44997732426305, 432.66902494331066, \"me valendo também nessas\"), Interval(432.66902494331066, 438.52045351473924, \"circunstâncias ahn e acredito até que grande parte de minhas viagens pelo estado de sao\"), Interval(438.52045351473924, 440.47092970521544, \"paulo principalmente nos\"), Interval(440.47092970521544, 441.9221768707483, \"pontos nao atendidos pelas companhias\"), Interval(441.9221768707483, 443.408253968254, \"de navegaçao aérea foram\"), Interval(443.408253968254, 444.8014512471655, \"feitos através do do do\"), Interval(444.8014512471655, 445.74185941043083, \"ônibus interestadual que\"), Interval(445.74185941043083, 446.3804081632653, \"eu detesto realmente apesar de\"), Interval(446.3804081632653, 448.22639455782314, \"\"), Interval(448.22639455782314, 448.8417233560091, \"\"), Interval(448.8417233560091, 450.2349206349206, \"\"), Interval(450.2349206349206, 450.81541950113376, \"\"), Interval(450.81541950113376, 452.0576870748299, \"\"), Interval(452.0576870748299, 454.9137414965986, \"\"), Interval(454.9137414965986, 456.51591836734696, \"\"), Interval(456.51591836734696, 457.65369614512474, \"\"), Interval(457.65369614512474, 458.4547845804989, \"\"), Interval(458.4547845804989, 460.37043083900227, \"\"), Interval(460.37043083900227, 463.4702947845805, \"\"), Interval(463.4702947845805, 465.0144217687075, \"\"), Interval(465.0144217687075, 466.2682993197279, \"\"), Interval(466.2682993197279, 470.02993197278914, \"\"), Interval(470.02993197278914, 472.06167800453517, \"\"), Interval(472.06167800453517, 474.52299319727894, \"\"), Interval(474.52299319727894, 477.46031746031747, \"\"), Interval(477.46031746031747, 479.60816326530613, \"\"), Interval(479.60816326530613, 481.0013605442177, \"\"), Interval(481.0013605442177, 483.3001360544218, \"\"), Interval(483.3001360544218, 486.0517006802721, \"\"), Interval(486.0517006802721, 489.5579138321996, \"\"), Interval(489.5579138321996, 491.5083900226757, \"\"), Interval(491.5083900226757, 492.7854875283447, \"\"), Interval(492.7854875283447, 493.60979591836735, \"\"), Interval(493.60979591836735, 494.60825396825396, \"\"), Interval(494.60825396825396, 496.52390022675735, \"\"), Interval(496.52390022675735, 499.05487528344673, \"\"), Interval(499.05487528344673, 501.58585034013606, \"reconhecer que alguns ahn nos\"), Interval(501.58585034013606, 509.1903854875283, \"ooferecem as condiçoes extremamente favoráveis de viagem mas ahn sou um indivíduo muito angustiado pelo fator tempo muito preocupado realmente\"), Interval(509.1903854875283, 513.49768707483, \"com o aproveitamento daquele tempo nao tenho o privilégio de acordar e me perguntar o que farei no dia de hoje né?\"), Interval(513.49768707483, 515.2740136054422, \"entao esta impossibilidade de me\"), Interval(515.2740136054422, 518.931156462585, \"valer assim do tempo faz com que as minhas experiências de transporte pelo ônibus sejam\"), Interval(518.931156462585, 519.372335600907, \"sempre experiências\"), Interval(519.372335600907, 524.6084353741496, \"marcadas por uma angustia de chegar e por uma preocupaçao muito grande de me me ver assim rapidamente desin-cumbido daquela tarefa o r\"), Interval(524.6084353741496, 526.7795011337869, \"deve ter andado de ônibus por aí bastante também  de navio nao sei\"), Interval(526.7795011337869, 549.9181859410431, \"\"), Interval(549.9181859410431, 550.48, \"em  vi mas nao\"), Interval(550.48, 558.4399092970522, \"\"), Interval(558.4399092970522, 559.6009070294784, \"você\"), Interval(559.6009070294784, 559.72, \"\"), Interval(559.72, 560.29, \"você devia\"), Interval(560.29, 561.01, \"\")])\n",
            "intervalo do metodo Interval(0.0, 3.47, \"\")\n",
            "intervalo do metodo Interval(3.47, 5.270929705215419, \"bom eu já viajei pelo país\")\n",
            "intervalo do metodo Interval(5.270929705215419, 5.6540589569161, \"inteiro\")\n",
            "intervalo do metodo Interval(5.6540589569161, 7.627755102040816, \"ahn dando cursos principalmente como\")\n",
            "intervalo do metodo Interval(7.627755102040816, 9.450521541950113, \"autor de livro didático tenho\")\n",
            "intervalo do metodo Interval(9.450521541950113, 11.877006802721088, \"a responsabilidade e até a incumbência de dar cursos promovidos\")\n",
            "intervalo do metodo Interval(11.877006802721088, 15.638639455782313, \"pelas minhas editoras e nessas circunstâncias eu\")\n",
            "intervalo do metodo Interval(15.638639455782313, 17.69360544217687, \"só nao estive no estado do acre\")\n",
            "intervalo do metodo Interval(17.69360544217687, 21.0140589569161, \"e nos territórios federais e naturalmente em todos os\")\n",
            "intervalo do metodo Interval(21.0140589569161, 21.96, \"demais\")\n",
            "intervalo do metodo Interval(21.96, 23.16190476190476, \"estados em\")\n",
            "intervalo do metodo Interval(23.16190476190476, 23.637913832199548, \"alguns\")\n",
            "intervalo do metodo Interval(23.637913832199548, 25.193650793650793, \"estados e muitas cidades\")\n",
            "intervalo do metodo Interval(25.193650793650793, 28.142585034013607, \"do interior e naturalmente na própria capital eh assim em termos\")\n",
            "intervalo do metodo Interval(28.142585034013607, 29.68671201814059, \"turisticos eu viajo muito raramente\")\n",
            "intervalo do metodo Interval(29.68671201814059, 31.207619047619048, \"até porque essa obrigatoriedade\")\n",
            "intervalo do metodo Interval(31.207619047619048, 35.247891156462586, \"de uma viagem sistemática quase todo ja-neiro em termos de\")\n",
            "intervalo do metodo Interval(35.247891156462586, 36.57142857142857, \"compromissos com as\")\n",
            "intervalo do metodo Interval(36.57142857142857, 38.81215419501134, \"editoras tira -disponibilidade de tempo e\")\n",
            "intervalo do metodo Interval(38.81215419501134, 39.19528344671202, \"até interesse em\")\n",
            "intervalo do metodo Interval(39.19528344671202, 40.06603174603175, \"viagens de cunho meramente\")\n",
            "intervalo do metodo Interval(40.06603174603175, 41.16897959183674, \"turístico o r nao sei se viaja  bastante ou igual proporçao\")\n",
            "intervalo do metodo Interval(41.16897959183674, 59.37342403628118, \"\")\n",
            "intervalo do metodo Interval(59.37342403628118, 63.75038548752835, \"é no meu caso ocorre uma circunstância curiosa ra-ramente eu passo mais de quarenta\")\n",
            "intervalo do metodo Interval(63.75038548752835, 64.33, \"e oito\")\n",
            "intervalo do metodo Interval(64.33, 69.65986394557824, \"horas no locais em que viajo por onde passo e dessas quarenta e\")\n",
            "intervalo do metodo Interval(69.65986394557824, 73.13124716553288, \"oito ho-ras umas dez eu passo dormindo e umas vinte dando cursos o que reduz assim o espaço para conhecer e  para realmente entrar em contato com o ambiente ahn muito pequeno ahn extremamente reduzido\")\n",
            "intervalo do metodo Interval(73.13124716553288, 99.92707482993197, \"\")\n",
            "intervalo do metodo Interval(99.92707482993197, 101.15773242630385, \"a minha última\")\n",
            "intervalo do metodo Interval(101.15773242630385, 102.43482993197279, \"viagem eu fiz uma cobertura de catorze estados\")\n",
            "intervalo do metodo Interval(102.43482993197279, 104.28081632653061, \"em dezessete dias de maneira que nessas\")\n",
            "intervalo do metodo Interval(104.28081632653061, 105.44181405895692, \"condiçoes só\")\n",
            "intervalo do metodo Interval(105.44181405895692, 108.59972789115646, \"poderia ter sido de aviao realmente o único trecho nao aéreo\")\n",
            "intervalo do metodo Interval(108.59972789115646, 110.80562358276644, \"em todo percurso foi um trecho fortaleza-teresina feito durante\")\n",
            "intervalo do metodo Interval(110.80562358276644, 114.07963718820862, \"a madrugada de maneira que o aviao acaba sendo\")\n",
            "intervalo do metodo Interval(114.07963718820862, 114.95038548752835, \"o meio de transporte único\")\n",
            "intervalo do metodo Interval(114.95038548752835, 115.71664399092971, \"possível nas atuais\")\n",
            "intervalo do metodo Interval(115.71664399092971, 117.99219954648527, \"circunstâncias mas também o preferido\")\n",
            "intervalo do metodo Interval(117.99219954648527, 119.21, \"por mim\")\n",
            "intervalo do metodo Interval(119.21, 120.56961451247166, \"eu realmente talvez pela circunstância em\")\n",
            "intervalo do metodo Interval(120.56961451247166, 121.67256235827665, \"que viajo venha  a preferir o aviao sobre outro meio\")\n",
            "intervalo do metodo Interval(121.67256235827665, 130.40326530612245, \"\")\n",
            "intervalo do metodo Interval(130.40326530612245, 133.32897959183674, \"bom\")\n",
            "intervalo do metodo Interval(133.32897959183674, 136.02249433106576, \"o atendimento é diretamente proporcional à extensao do do\")\n",
            "intervalo do metodo Interval(136.02249433106576, 140.3065759637188, \"vôo e naturalmente o preço da passagem ahn eu quero crer que uma viagem sao paulo e manaus ou sao paulo a belém a gente\")\n",
            "intervalo do metodo Interval(140.3065759637188, 145.345306122449, \"costuma ser muito bem atendido e regiamente tratado de maneira assim toda especial em termos particulares eu\")\n",
            "intervalo do metodo Interval(145.345306122449, 146.90104308390022, \"prefiro a vasp sempre me dei muito bem com\")\n",
            "intervalo do metodo Interval(146.90104308390022, 148.59609977324263, \"o tratamento da vasp mas nao nunca fui\")\n",
            "intervalo do metodo Interval(148.59609977324263, 149.37396825396826, \"mal\")\n",
            "intervalo do metodo Interval(149.37396825396826, 152.89179138321995, \"tratado ou mal recebido em outras companhias areas agora numa viagem\")\n",
            "intervalo do metodo Interval(152.89179138321995, 154.7261678004535, \"daqui ao rio de janeiro que é uma viagem quase que\")\n",
            "intervalo do metodo Interval(154.7261678004535, 157.68671201814058, \"habitual e comumaté o cafezinho de péssima qualidade está se\")\n",
            "intervalo do metodo Interval(157.68671201814058, 159.1960090702948, \"tornando cada vez mais difícil de maneira que\")\n",
            "intervalo do metodo Interval(159.1960090702948, 161.75, \"eu suponho que nesta proporçao e mesmo em contato com\")\n",
            "intervalo do metodo Interval(161.75, 161.9243537414966, \"outras\")\n",
            "intervalo do metodo Interval(161.9243537414966, 163.97931972789115, \"pessoas que tiveram viagens internacionais que\")\n",
            "intervalo do metodo Interval(163.97931972789115, 165.70920634920634, \"à medida que vai a distância aumentando vai\")\n",
            "intervalo do metodo Interval(165.70920634920634, 168.98321995464852, \"naturalmente aumentando o preço da passagem em funçao disso a qualificaçao do  tratamento que aí no caso acaba se tornando muito boa e muito produtiva\")\n",
            "intervalo do metodo Interval(168.98321995464852, 214.98195011337867, \"\")\n",
            "intervalo do metodo Interval(214.98195011337867, 217.4548752834467, \"comigo é o contrário eu talvez\")\n",
            "intervalo do metodo Interval(217.4548752834467, 221.48353741496598, \"até por um certo exotismo gostaria de poder de contar alguma experiência\")\n",
            "intervalo do metodo Interval(221.48353741496598, 223.06, \"aérea traumatizante ou pelo menos inquietante\")\n",
            "intervalo do metodo Interval(223.06, 223.65, \"mas\")\n",
            "intervalo do metodo Interval(223.65, 225.93015873015872, \"a nao ser alguns atrasos homéricos né? excepcionais\")\n",
            "intervalo do metodo Interval(225.93015873015872, 230.06331065759636, \"assim em termos de viagens aparentemente de pequena distância nunca\")\n",
            "intervalo do metodo Interval(230.06331065759636, 231.61904761904762, \"encontrei circunstâncias que tenham me\")\n",
            "intervalo do metodo Interval(231.61904761904762, 236.42557823129252, \"feito preocupar ou temer pela própria segurança nunca\")\n",
            "intervalo do metodo Interval(236.42557823129252, 236.9596371882086, \"\")\n",
            "intervalo do metodo Interval(236.9596371882086, 245.95736961451246, \"\")\n",
            "intervalo do metodo Interval(245.95736961451246, 246.46820861678003, \"\")\n",
            "intervalo do metodo Interval(246.46820861678003, 246.94421768707483, \"\")\n",
            "intervalo do metodo Interval(246.94421768707483, 249.27782312925171, \"\")\n",
            "intervalo do metodo Interval(249.27782312925171, 251.5998185941043, \"\")\n",
            "intervalo do metodo Interval(251.5998185941043, 252.11065759637188, \"\")\n",
            "intervalo do metodo Interval(252.11065759637188, 252.55183673469386, \"\")\n",
            "intervalo do metodo Interval(252.55183673469386, 258.7747845804989, \"\")\n",
            "intervalo do metodo Interval(258.7747845804989, 260.42340136054423, \"\")\n",
            "intervalo do metodo Interval(260.42340136054423, 264.8003628117914, \"\")\n",
            "intervalo do metodo Interval(264.8003628117914, 270.0132426303855, \"\")\n",
            "intervalo do metodo Interval(270.0132426303855, 273.57750566893424, \"\")\n",
            "intervalo do metodo Interval(273.57750566893424, 279.2083446712018, \"\")\n",
            "intervalo do metodo Interval(279.2083446712018, 280.4738321995465, \"\")\n",
            "intervalo do metodo Interval(280.4738321995465, 292.3740589569161, \"\")\n",
            "intervalo do metodo Interval(292.3740589569161, 295.6248526077097, \"\")\n",
            "intervalo do metodo Interval(295.6248526077097, 296.10086167800455, \"\")\n",
            "intervalo do metodo Interval(296.10086167800455, 300.930612244898, \"\")\n",
            "intervalo do metodo Interval(300.930612244898, 301.99873015873015, \"\")\n",
            "intervalo do metodo Interval(301.99873015873015, 302.5792290249433, \"\")\n",
            "intervalo do metodo Interval(302.5792290249433, 304.1814058956916, \"\")\n",
            "intervalo do metodo Interval(304.1814058956916, 311.1241723356009, \"\")\n",
            "intervalo do metodo Interval(311.1241723356009, 313.7596371882086, \"\")\n",
            "intervalo do metodo Interval(313.7596371882086, 318.24108843537414, \"\")\n",
            "intervalo do metodo Interval(318.24108843537414, 320.6675736961451, \"\")\n",
            "intervalo do metodo Interval(320.6675736961451, 323.2914285714286, \"\")\n",
            "intervalo do metodo Interval(323.2914285714286, 324.11573696145126, \"\")\n",
            "intervalo do metodo Interval(324.11573696145126, 324.9168253968254, \"\")\n",
            "intervalo do metodo Interval(324.9168253968254, 327.9121995464853, \"\")\n",
            "intervalo do metodo Interval(327.9121995464853, 329.3053968253968, \"\")\n",
            "intervalo do metodo Interval(329.3053968253968, 333.47337868480724, \"\")\n",
            "intervalo do metodo Interval(333.47337868480724, 334.5298866213152, \"\")\n",
            "intervalo do metodo Interval(334.5298866213152, 337.7922902494331, \"\")\n",
            "intervalo do metodo Interval(337.7922902494331, 338.53532879818596, \"\")\n",
            "intervalo do metodo Interval(338.53532879818596, 342.86585034013603, \"tive aquela tao característica posiçao do indivíduo que desce e\")\n",
            "intervalo do metodo Interval(342.86585034013603, 344.2358276643991, \"\")\n",
            "intervalo do metodo Interval(344.2358276643991, 344.75827664399094, \"beija a\")\n",
            "intervalo do metodo Interval(344.75827664399094, 346.60426303854877, \"terra agradecido sempre as minhas\")\n",
            "intervalo do metodo Interval(346.60426303854877, 350.5516553287982, \"experiências aéreas foram das mais favoráveis possíveis como particularidade curiosa das minhas viagens aéreas eu suponho que ahn\")\n",
            "intervalo do metodo Interval(350.5516553287982, 353.059410430839, \"a mais exótica foi o fato de ter trocado\")\n",
            "intervalo do metodo Interval(353.059410430839, 355.5787755102041, \"o valor de uma passagem por uma palestra a oito\")\n",
            "intervalo do metodo Interval(355.5787755102041, 357.68018140589567, \"mil metros de altitude quando do vôo inaugural da vasp para manaus ahn\")\n",
            "intervalo do metodo Interval(357.68018140589567, 360.4665759637188, \"o vôo do one eleven' um dos diretores\")\n",
            "intervalo do metodo Interval(360.4665759637188, 361.01, \"da companhia\")\n",
            "intervalo do metodo Interval(361.01, 361.67, \"me\")\n",
            "intervalo do metodo Interval(361.67, 364.5532879818594, \"propôs como forma de promoçao uma passagem aérea gratuivta eu eu disse a\")\n",
            "intervalo do metodo Interval(364.5532879818594, 366.48054421768705, \"ele que só poderia aceitar se fosse possível\")\n",
            "intervalo do metodo Interval(366.48054421768705, 368.7096598639456, \"levar minha esposa\")\n",
            "intervalo do metodo Interval(368.7096598639456, 372.3784126984127, \"tambem entao ele disse que nessa circunstância para justificar perante a companhia a ida da\")\n",
            "intervalo do metodo Interval(372.3784126984127, 376.1168253968254, \"minha minha esposa eu teria que fazer alguma coisa pela companhia e a aquilo\")\n",
            "intervalo do metodo Interval(376.1168253968254, 376.90630385487526, \"que o professor sabe fazer ahn\")\n",
            "intervalo do metodo Interval(376.90630385487526, 379.5069387755102, \"única e exclusivamente é dar\")\n",
            "intervalo do metodo Interval(379.5069387755102, 381.85215419501134, \"aula entao ele pediu que eu preparasse uma aula\")\n",
            "intervalo do metodo Interval(381.85215419501134, 384.67337868480723, \"para apresentar aos passageiros através do do microfone de de bordo ahn\")\n",
            "intervalo do metodo Interval(384.67337868480723, 385.8459863945578, \"numa grande altitude\")\n",
            "intervalo do metodo Interval(385.8459863945578, 388.87619047619046, \"e foi uma\")\n",
            "intervalo do metodo Interval(388.87619047619046, 390.69895691609975, \"palestra de oito minutos nao mais do\")\n",
            "intervalo do metodo Interval(390.69895691609975, 392.3591836734694, \"que isso sobre a ocupaçao amazônica do interesse\")\n",
            "intervalo do metodo Interval(392.3591836734694, 394.24, \"internacional pela regiao e o\")\n",
            "intervalo do metodo Interval(394.24, 398.3151020408163, \"de exótico é que acredito que poucas pessoas tiveram o privilégio que tive de dar\")\n",
            "intervalo do metodo Interval(398.3151020408163, 400.78802721088437, \"uma palestra a oito\")\n",
            "intervalo do metodo Interval(400.78802721088437, 402.14639455782316, \"mil metros de  altitude\")\n",
            "intervalo do metodo Interval(402.14639455782316, 410.64489795918365, \"\")\n",
            "intervalo do metodo Interval(410.64489795918365, 412.9320634920635, \"as minhas experiências marítimas sao extremamente\")\n",
            "intervalo do metodo Interval(412.9320634920635, 416.27573696145123, \"limitadas eu ahn digamos assim como passeios para conheci-mento da linha náutica\")\n",
            "intervalo do metodo Interval(416.27573696145123, 417.2974149659864, \"o oferta assim do ministério\")\n",
            "intervalo do metodo Interval(417.2974149659864, 418.76027210884354, \"da marinha algumas\")\n",
            "intervalo do metodo Interval(418.76027210884354, 419.06, \"\")\n",
            "intervalo do metodo Interval(419.06, 419.07374149659864, \"vezes\")\n",
            "intervalo do metodo Interval(419.07374149659864, 420.01, \"\")\n",
            "intervalo do metodo Interval(420.01, 421.07065759637186, \"eu tive a oportunidade\")\n",
            "intervalo do metodo Interval(421.07065759637186, 421.5698866213152, \"\")\n",
            "intervalo do metodo Interval(421.5698866213152, 422.11555555555555, \"\")\n",
            "intervalo do metodo Interval(422.11555555555555, 424.9251700680272, \"da fazer ahn a uma incursao pelo mar muito pequena\")\n",
            "intervalo do metodo Interval(424.9251700680272, 426.2254875283447, \"o transporte urbano e mesmo o\")\n",
            "intervalo do metodo Interval(426.2254875283447, 426.991746031746, \"\")\n",
            "intervalo do metodo Interval(426.991746031746, 428.76807256235827, \"transporte suburbano e às vezes até\")\n",
            "intervalo do metodo Interval(428.76807256235827, 431.44997732426305, \"interestadual do ônibus muitas vezes acabei\")\n",
            "intervalo do metodo Interval(431.44997732426305, 432.66902494331066, \"me valendo também nessas\")\n",
            "intervalo do metodo Interval(432.66902494331066, 438.52045351473924, \"circunstâncias ahn e acredito até que grande parte de minhas viagens pelo estado de sao\")\n",
            "intervalo do metodo Interval(438.52045351473924, 440.47092970521544, \"paulo principalmente nos\")\n",
            "intervalo do metodo Interval(440.47092970521544, 441.9221768707483, \"pontos nao atendidos pelas companhias\")\n",
            "intervalo do metodo Interval(441.9221768707483, 443.408253968254, \"de navegaçao aérea foram\")\n",
            "intervalo do metodo Interval(443.408253968254, 444.8014512471655, \"feitos através do do do\")\n",
            "intervalo do metodo Interval(444.8014512471655, 445.74185941043083, \"ônibus interestadual que\")\n",
            "intervalo do metodo Interval(445.74185941043083, 446.3804081632653, \"eu detesto realmente apesar de\")\n",
            "intervalo do metodo Interval(446.3804081632653, 448.22639455782314, \"\")\n",
            "intervalo do metodo Interval(448.22639455782314, 448.8417233560091, \"\")\n",
            "intervalo do metodo Interval(448.8417233560091, 450.2349206349206, \"\")\n",
            "intervalo do metodo Interval(450.2349206349206, 450.81541950113376, \"\")\n",
            "intervalo do metodo Interval(450.81541950113376, 452.0576870748299, \"\")\n",
            "intervalo do metodo Interval(452.0576870748299, 454.9137414965986, \"\")\n",
            "intervalo do metodo Interval(454.9137414965986, 456.51591836734696, \"\")\n",
            "intervalo do metodo Interval(456.51591836734696, 457.65369614512474, \"\")\n",
            "intervalo do metodo Interval(457.65369614512474, 458.4547845804989, \"\")\n",
            "intervalo do metodo Interval(458.4547845804989, 460.37043083900227, \"\")\n",
            "intervalo do metodo Interval(460.37043083900227, 463.4702947845805, \"\")\n",
            "intervalo do metodo Interval(463.4702947845805, 465.0144217687075, \"\")\n",
            "intervalo do metodo Interval(465.0144217687075, 466.2682993197279, \"\")\n",
            "intervalo do metodo Interval(466.2682993197279, 470.02993197278914, \"\")\n",
            "intervalo do metodo Interval(470.02993197278914, 472.06167800453517, \"\")\n",
            "intervalo do metodo Interval(472.06167800453517, 474.52299319727894, \"\")\n",
            "intervalo do metodo Interval(474.52299319727894, 477.46031746031747, \"\")\n",
            "intervalo do metodo Interval(477.46031746031747, 479.60816326530613, \"\")\n",
            "intervalo do metodo Interval(479.60816326530613, 481.0013605442177, \"\")\n",
            "intervalo do metodo Interval(481.0013605442177, 483.3001360544218, \"\")\n",
            "intervalo do metodo Interval(483.3001360544218, 486.0517006802721, \"\")\n",
            "intervalo do metodo Interval(486.0517006802721, 489.5579138321996, \"\")\n",
            "intervalo do metodo Interval(489.5579138321996, 491.5083900226757, \"\")\n",
            "intervalo do metodo Interval(491.5083900226757, 492.7854875283447, \"\")\n",
            "intervalo do metodo Interval(492.7854875283447, 493.60979591836735, \"\")\n",
            "intervalo do metodo Interval(493.60979591836735, 494.60825396825396, \"\")\n",
            "intervalo do metodo Interval(494.60825396825396, 496.52390022675735, \"\")\n",
            "intervalo do metodo Interval(496.52390022675735, 499.05487528344673, \"\")\n",
            "intervalo do metodo Interval(499.05487528344673, 501.58585034013606, \"reconhecer que alguns ahn nos\")\n",
            "intervalo do metodo Interval(501.58585034013606, 509.1903854875283, \"ooferecem as condiçoes extremamente favoráveis de viagem mas ahn sou um indivíduo muito angustiado pelo fator tempo muito preocupado realmente\")\n",
            "intervalo do metodo Interval(509.1903854875283, 513.49768707483, \"com o aproveitamento daquele tempo nao tenho o privilégio de acordar e me perguntar o que farei no dia de hoje né?\")\n",
            "intervalo do metodo Interval(513.49768707483, 515.2740136054422, \"entao esta impossibilidade de me\")\n",
            "intervalo do metodo Interval(515.2740136054422, 518.931156462585, \"valer assim do tempo faz com que as minhas experiências de transporte pelo ônibus sejam\")\n",
            "intervalo do metodo Interval(518.931156462585, 519.372335600907, \"sempre experiências\")\n",
            "intervalo do metodo Interval(519.372335600907, 524.6084353741496, \"marcadas por uma angustia de chegar e por uma preocupaçao muito grande de me me ver assim rapidamente desin-cumbido daquela tarefa o r\")\n",
            "intervalo do metodo Interval(524.6084353741496, 526.7795011337869, \"deve ter andado de ônibus por aí bastante também  de navio nao sei\")\n",
            "intervalo do metodo Interval(526.7795011337869, 549.9181859410431, \"\")\n",
            "intervalo do metodo Interval(549.9181859410431, 550.48, \"em  vi mas nao\")\n",
            "intervalo do metodo Interval(550.48, 558.4399092970522, \"\")\n",
            "intervalo do metodo Interval(558.4399092970522, 559.6009070294784, \"você\")\n",
            "intervalo do metodo Interval(559.6009070294784, 559.72, \"\")\n",
            "intervalo do metodo Interval(559.72, 560.29, \"você devia\")\n",
            "intervalo do metodo Interval(560.29, 561.01, \"\")\n",
            "tier NTB-l2 adicionada: IntervalTier(start_time=0.0, end_time=561.01, name=\"NTB-l2\", objects=[Interval(0.0, 41.16897959183674, \"\"), Interval(41.16897959183674, 43.456145124716556, \"minhas viagens sao bem\"), Interval(43.456145124716556, 44.651972789115646, \"diferentes eu viajo mais é fazendo turismo\"), Interval(44.651972789115646, 47.484807256235825, \"mesmo boto a família no carro e saio\"), Interval(47.484807256235825, 47.65, \"\"), Interval(47.65, 47.925986394557825, \"por\"), Interval(47.925986394557825, 49.621043083900226, \"aí nao é? fiz assim três viagens nessa base sendo\"), Interval(49.621043083900226, 53.0227664399093, \"que uma para o rio grande do sul um mês inteiro depois\"), Interval(53.0227664399093, 57.50421768707483, \"vinte dias na bahia no ano seguinte e quinze dias no espírito santo e estado do rio essas foram as\"), Interval(57.50421768707483, 59.37342403628118, \"as viagens melhores que eu fiz assim maiores o  mais é viagem curta aí perto essa coisa\"), Interval(59.37342403628118, 82.581768707483, \"\"), Interval(82.581768707483, 84.7760544217687, \"nao eu viajo sempre de automóvel porque\"), Interval(84.7760544217687, 86.2853514739229, \"sao seis filhos entao nao dá\"), Interval(86.2853514739229, 87.24897959183673, \"para viajar de outro\"), Interval(87.24897959183673, 88.25904761904762, \"jeito se fica crianças agora que é\"), Interval(88.25904761904762, 88.94, \"que é uhn\"), Interval(88.94, 89.50131519274376, \"uhn o caçula tem\"), Interval(89.50131519274376, 92.13678004535147, \"oito anos nao é e poderíamos pensar em um outro transporte porque até\"), Interval(92.13678004535147, 93.70412698412699, \"agora o automóvel ajuda\"), Interval(93.70412698412699, 95.64299319727891, \"muito né? com criança pequena e depois é\"), Interval(95.64299319727891, 97.41931972789115, \"divertido né? enfiar a família toda\"), Interval(97.41931972789115, 98.41, \"no carro sair\"), Interval(98.41, 99.92707482993197, \"andando por aí é muito bom tenho viajado  sempre de automóvel\"), Interval(99.92707482993197, 173.56, \"\"), Interval(173.56, 176.07691609977323, \"já viajei de aviao há dez anos atrás eu trabalhava com\"), Interval(176.07691609977323, 178.5150113378685, \"um jornal e nao era professor entao\"), Interval(178.5150113378685, 179.0722902494331, \"minha atividade\"), Interval(179.0722902494331, 179.8965986394558, \"era\"), Interval(179.8965986394558, 180.41904761904763, \"mais diversificada\"), Interval(180.41904761904763, 181.3362358276644, \"\"), Interval(181.3362358276644, 183.00807256235828, \"entao viajei bastante de aviao andei muito\"), Interval(183.00807256235828, 183.9484807256236, \"também por\"), Interval(183.9484807256236, 185.6087074829932, \"aí afora de aviao e mas nao gostava\"), Interval(185.6087074829932, 186.90902494331067, \"assim como parece que o c gosta né c?\"), Interval(186.90902494331067, 188.09, \"quer dizer\"), Interval(188.09, 188.79, \"minhas viagens\"), Interval(188.79, 190.43845804988663, \"de aviao eram\"), Interval(190.43845804988663, 193.9911111111111, \"mesmo por negócio estritamente quando eu podia fazer viagem de automóvel\"), Interval(193.9911111111111, 194.4903401360544, \"ou por\"), Interval(194.4903401360544, 194.80380952380952, \"outro meio\"), Interval(194.80380952380952, 196.04607709750567, \"eu prefi sempre\"), Interval(196.04607709750567, 199.00662131519275, \"dei preferência talvez há dez anos\"), Interval(199.00662131519275, 200.60879818594105, \"atrás os avioes nao tinham\"), Interval(200.60879818594105, 202.02521541950114, \"o conforto de hoje e eu\"), Interval(202.02521541950114, 205.28761904761905, \"tive uma experiência assim meio desagradável numa viagem de um viscount\"), Interval(205.28761904761905, 207.89986394557823, \"da vasp até por sinal pegamos um desses cb que eles chamam né? e o\"), Interval(207.89986394557823, 210.62820861678006, \"aviao quase caiu comigo lá\"), Interval(210.62820861678006, 211.22031746031746, \"perto de goiânia né\"), Interval(211.22031746031746, 213.01986394557824, \"e entao isso também colaborou para\"), Interval(213.01986394557824, 214.98195011337867, \"que eu nao tivesse muito entusiasmo  em viagens aéreas sabe?\"), Interval(214.98195011337867, 526.7795011337869, \"\"), Interval(526.7795011337869, 529.1363265306122, \"éh eu realmente\"), Interval(529.1363265306122, 530.8778231292517, \"tenho parentes no rio na guanabara entao\"), Interval(530.8778231292517, 532.1897505668934, \"eu ando muito de ônibus daqui para o rio hoje nem\"), Interval(532.1897505668934, 544.8446258503401, \"tanto mas há algum tempo atrás andava bastante era raro o mês ou cada dois meses que eu nao ia de ônibus ou de trem mesmo até a guanabara eu viajei muito daquele de santa cruz aquele noturno né? várias vezes e por mar apenas aquelas barcas da cantareira quando eu estava no rio eu ia lá dá na\"), Interval(544.8446258503401, 545.65, \"praça xv até\"), Interval(545.65, 546.06, \"paquetá\"), Interval(546.06, 546.516462585034, \"fui muito a paquetá\"), Interval(546.516462585034, 549.9181859410431, \"de naquelas barcas da cantareira que lembram aquelas barcas do mississipi né? com aquelas  rodas laterais\"), Interval(549.9181859410431, 550.48, \"\"), Interval(550.48, 552.5768707482994, \"por sinal que é uma delícia aquilo\"), Interval(552.5768707482994, 553.9700680272109, \"sabe? aquilo me dá uma saudade imensa o tempo que era jovem porque\"), Interval(553.9700680272109, 554.7014965986394, \"todas as férias eu ia\"), Interval(554.7014965986394, 555.6419047619048, \"em paquetá\"), Interval(555.6419047619048, 556.1179138321995, \"entao\"), Interval(556.1179138321995, 556.77, \"eu\"), Interval(556.77, 558.4399092970522, \"ia daqui para o rio de  trem e lá\"), Interval(558.4399092970522, 561.01, \"\")])\n",
            "intervalo do metodo Interval(0.0, 41.16897959183674, \"\")\n",
            "intervalo do metodo Interval(41.16897959183674, 43.456145124716556, \"minhas viagens sao bem\")\n",
            "intervalo do metodo Interval(43.456145124716556, 44.651972789115646, \"diferentes eu viajo mais é fazendo turismo\")\n",
            "intervalo do metodo Interval(44.651972789115646, 47.484807256235825, \"mesmo boto a família no carro e saio\")\n",
            "intervalo do metodo Interval(47.484807256235825, 47.65, \"\")\n",
            "intervalo do metodo Interval(47.65, 47.925986394557825, \"por\")\n",
            "intervalo do metodo Interval(47.925986394557825, 49.621043083900226, \"aí nao é? fiz assim três viagens nessa base sendo\")\n",
            "intervalo do metodo Interval(49.621043083900226, 53.0227664399093, \"que uma para o rio grande do sul um mês inteiro depois\")\n",
            "intervalo do metodo Interval(53.0227664399093, 57.50421768707483, \"vinte dias na bahia no ano seguinte e quinze dias no espírito santo e estado do rio essas foram as\")\n",
            "intervalo do metodo Interval(57.50421768707483, 59.37342403628118, \"as viagens melhores que eu fiz assim maiores o  mais é viagem curta aí perto essa coisa\")\n",
            "intervalo do metodo Interval(59.37342403628118, 82.581768707483, \"\")\n",
            "intervalo do metodo Interval(82.581768707483, 84.7760544217687, \"nao eu viajo sempre de automóvel porque\")\n",
            "intervalo do metodo Interval(84.7760544217687, 86.2853514739229, \"sao seis filhos entao nao dá\")\n",
            "intervalo do metodo Interval(86.2853514739229, 87.24897959183673, \"para viajar de outro\")\n",
            "intervalo do metodo Interval(87.24897959183673, 88.25904761904762, \"jeito se fica crianças agora que é\")\n",
            "intervalo do metodo Interval(88.25904761904762, 88.94, \"que é uhn\")\n",
            "intervalo do metodo Interval(88.94, 89.50131519274376, \"uhn o caçula tem\")\n",
            "intervalo do metodo Interval(89.50131519274376, 92.13678004535147, \"oito anos nao é e poderíamos pensar em um outro transporte porque até\")\n",
            "intervalo do metodo Interval(92.13678004535147, 93.70412698412699, \"agora o automóvel ajuda\")\n",
            "intervalo do metodo Interval(93.70412698412699, 95.64299319727891, \"muito né? com criança pequena e depois é\")\n",
            "intervalo do metodo Interval(95.64299319727891, 97.41931972789115, \"divertido né? enfiar a família toda\")\n",
            "intervalo do metodo Interval(97.41931972789115, 98.41, \"no carro sair\")\n",
            "intervalo do metodo Interval(98.41, 99.92707482993197, \"andando por aí é muito bom tenho viajado  sempre de automóvel\")\n",
            "intervalo do metodo Interval(99.92707482993197, 173.56, \"\")\n",
            "intervalo do metodo Interval(173.56, 176.07691609977323, \"já viajei de aviao há dez anos atrás eu trabalhava com\")\n",
            "intervalo do metodo Interval(176.07691609977323, 178.5150113378685, \"um jornal e nao era professor entao\")\n",
            "intervalo do metodo Interval(178.5150113378685, 179.0722902494331, \"minha atividade\")\n",
            "intervalo do metodo Interval(179.0722902494331, 179.8965986394558, \"era\")\n",
            "intervalo do metodo Interval(179.8965986394558, 180.41904761904763, \"mais diversificada\")\n",
            "intervalo do metodo Interval(180.41904761904763, 181.3362358276644, \"\")\n",
            "intervalo do metodo Interval(181.3362358276644, 183.00807256235828, \"entao viajei bastante de aviao andei muito\")\n",
            "intervalo do metodo Interval(183.00807256235828, 183.9484807256236, \"também por\")\n",
            "intervalo do metodo Interval(183.9484807256236, 185.6087074829932, \"aí afora de aviao e mas nao gostava\")\n",
            "intervalo do metodo Interval(185.6087074829932, 186.90902494331067, \"assim como parece que o c gosta né c?\")\n",
            "intervalo do metodo Interval(186.90902494331067, 188.09, \"quer dizer\")\n",
            "intervalo do metodo Interval(188.09, 188.79, \"minhas viagens\")\n",
            "intervalo do metodo Interval(188.79, 190.43845804988663, \"de aviao eram\")\n",
            "intervalo do metodo Interval(190.43845804988663, 193.9911111111111, \"mesmo por negócio estritamente quando eu podia fazer viagem de automóvel\")\n",
            "intervalo do metodo Interval(193.9911111111111, 194.4903401360544, \"ou por\")\n",
            "intervalo do metodo Interval(194.4903401360544, 194.80380952380952, \"outro meio\")\n",
            "intervalo do metodo Interval(194.80380952380952, 196.04607709750567, \"eu prefi sempre\")\n",
            "intervalo do metodo Interval(196.04607709750567, 199.00662131519275, \"dei preferência talvez há dez anos\")\n",
            "intervalo do metodo Interval(199.00662131519275, 200.60879818594105, \"atrás os avioes nao tinham\")\n",
            "intervalo do metodo Interval(200.60879818594105, 202.02521541950114, \"o conforto de hoje e eu\")\n",
            "intervalo do metodo Interval(202.02521541950114, 205.28761904761905, \"tive uma experiência assim meio desagradável numa viagem de um viscount\")\n",
            "intervalo do metodo Interval(205.28761904761905, 207.89986394557823, \"da vasp até por sinal pegamos um desses cb que eles chamam né? e o\")\n",
            "intervalo do metodo Interval(207.89986394557823, 210.62820861678006, \"aviao quase caiu comigo lá\")\n",
            "intervalo do metodo Interval(210.62820861678006, 211.22031746031746, \"perto de goiânia né\")\n",
            "intervalo do metodo Interval(211.22031746031746, 213.01986394557824, \"e entao isso também colaborou para\")\n",
            "intervalo do metodo Interval(213.01986394557824, 214.98195011337867, \"que eu nao tivesse muito entusiasmo  em viagens aéreas sabe?\")\n",
            "intervalo do metodo Interval(214.98195011337867, 526.7795011337869, \"\")\n",
            "intervalo do metodo Interval(526.7795011337869, 529.1363265306122, \"éh eu realmente\")\n",
            "intervalo do metodo Interval(529.1363265306122, 530.8778231292517, \"tenho parentes no rio na guanabara entao\")\n",
            "intervalo do metodo Interval(530.8778231292517, 532.1897505668934, \"eu ando muito de ônibus daqui para o rio hoje nem\")\n",
            "intervalo do metodo Interval(532.1897505668934, 544.8446258503401, \"tanto mas há algum tempo atrás andava bastante era raro o mês ou cada dois meses que eu nao ia de ônibus ou de trem mesmo até a guanabara eu viajei muito daquele de santa cruz aquele noturno né? várias vezes e por mar apenas aquelas barcas da cantareira quando eu estava no rio eu ia lá dá na\")\n",
            "intervalo do metodo Interval(544.8446258503401, 545.65, \"praça xv até\")\n",
            "intervalo do metodo Interval(545.65, 546.06, \"paquetá\")\n",
            "intervalo do metodo Interval(546.06, 546.516462585034, \"fui muito a paquetá\")\n",
            "intervalo do metodo Interval(546.516462585034, 549.9181859410431, \"de naquelas barcas da cantareira que lembram aquelas barcas do mississipi né? com aquelas  rodas laterais\")\n",
            "intervalo do metodo Interval(549.9181859410431, 550.48, \"\")\n",
            "intervalo do metodo Interval(550.48, 552.5768707482994, \"por sinal que é uma delícia aquilo\")\n",
            "intervalo do metodo Interval(552.5768707482994, 553.9700680272109, \"sabe? aquilo me dá uma saudade imensa o tempo que era jovem porque\")\n",
            "intervalo do metodo Interval(553.9700680272109, 554.7014965986394, \"todas as férias eu ia\")\n",
            "intervalo do metodo Interval(554.7014965986394, 555.6419047619048, \"em paquetá\")\n",
            "intervalo do metodo Interval(555.6419047619048, 556.1179138321995, \"entao\")\n",
            "intervalo do metodo Interval(556.1179138321995, 556.77, \"eu\")\n",
            "intervalo do metodo Interval(556.77, 558.4399092970522, \"ia daqui para o rio de  trem e lá\")\n",
            "intervalo do metodo Interval(558.4399092970522, 561.01, \"\")\n",
            "tier NTB-coloca adicionada: IntervalTier(start_time=0.0, end_time=561.01, name=\"NTB-coloca\", objects=[Interval(0.0, 80.78, \"\"), Interval(80.78, 80.94, \"a\"), Interval(80.94, 82.581768707483, \"família no carro e vai viajar o senhor usa um outro meio  de transporte?\"), Interval(82.581768707483, 561.01, \"\")])\n",
            "intervalo do metodo Interval(0.0, 80.78, \"\")\n",
            "intervalo do metodo Interval(80.78, 80.94, \"a\")\n",
            "intervalo do metodo Interval(80.94, 82.581768707483, \"família no carro e vai viajar o senhor usa um outro meio  de transporte?\")\n",
            "intervalo do metodo Interval(82.581768707483, 561.01, \"\")\n",
            "acertos: 68 / 293 = 0.23208191126279865\n",
            "métrica SER: (I+R)/(C+R) 1.000952805953228\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.000952805953228"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}
